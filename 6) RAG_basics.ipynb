{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96793c9a",
   "metadata": {},
   "source": [
    "As it is (without anything downloaded and always with 1 epoch when needed), the running time of the whole notebook is (approximately) <span style=\"background-color: lightblue\"> 1 minutes</span>.\n",
    "\n",
    "<span style=\"background-color: yellow\"> </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4e6555",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b445c2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/35/gw8dmgsd6m11bg8nhrgpd3vr0000gn/T/ipykernel_19920/762308708.py:15: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources            # package and dependency management\n",
      "/opt/miniconda3/envs/Trans_env/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/miniconda3/envs/Trans_env/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/miniconda3/envs/Trans_env/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/miniconda3/envs/Trans_env/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "########################## UTILITY AND SYSTEM ##########################\n",
    "\n",
    "import os                       # filesystem operations\n",
    "import csv                      # reading/writing CSV files\n",
    "import json                     # JSON parsing and serialization\n",
    "import math                     # basic math functions\n",
    "import random                   # random number generation\n",
    "import time                     # time-related functions\n",
    "import tempfile                 # temporary file management\n",
    "import tarfile                  # tar archive handling\n",
    "import io                       # input/output streams\n",
    "import pickle                   # object serialization\n",
    "import importlib                # dynamic import of modules\n",
    "import multiprocessing          # parallel process management\n",
    "import pkg_resources            # package and dependency management\n",
    "from copy import deepcopy       # deep copy of objects\n",
    "from pathlib import Path        # filesystem paths handling (cross-platform)\n",
    "\n",
    "########################## DOWNLOAD ##########################\n",
    "\n",
    "import requests                 # HTTP requests library\n",
    "import wget                     # file downloads from URLs\n",
    "from urllib.request import urlopen  # open URLs (alternative to requests)\n",
    "\n",
    "########################## VISUALIZATION ##########################\n",
    "\n",
    "import matplotlib.pyplot as plt # basic plotting library\n",
    "import plotly.graph_objs as go  # interactive plotting\n",
    "from tqdm.notebook import tqdm  # progress bars for loops in notebooks\n",
    "from pprint import pprint       # formatted pretty-printing of objects\n",
    "\n",
    "########################## DATAFRAME ##########################\n",
    "\n",
    "import numpy as np              # numerical arrays and operations\n",
    "import pandas as pd             # dataframes and data manipulation\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "########################## TEXT PROCESSING ##########################\n",
    "\n",
    "import re                      # regular expressions\n",
    "import string                  # string constants and operations\n",
    "from itertools import chain, islice  # advanced iteration and chaining\n",
    "\n",
    "########################## TOKENIZATION ##########################\n",
    "\n",
    "from collections import Counter, OrderedDict  # frequency counts and ordered dictionaries\n",
    "import nltk                                   # natural language processing toolkit\n",
    "from nltk.tokenize import word_tokenize       # word tokenization\n",
    "import spacy                                  # advanced NLP (tokenization, parsing)\n",
    "from torchtext.data.utils import get_tokenizer       # torchtext tokenizers\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator # build vocabulary from iterator\n",
    "\n",
    "########################## DATASET AND DATALOADER ##########################\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split   # datasets and data loading utilities\n",
    "from torch.nn.utils.rnn import pad_sequence                      # padding variable-length sequences\n",
    "from datasets import load_dataset, DatasetDict                   # HuggingFace datasets loading\n",
    "from torchtext.datasets import AG_NEWS                           # torchtext built-in datasets\n",
    "\n",
    "########################## PYTORCH AND DEEP LEARNING ##########################\n",
    "\n",
    "import torch                             # PyTorch main library\n",
    "from torch import nn, Tensor             # neural network modules and tensors\n",
    "from torch.nn import CrossEntropyLoss    # common loss function for classification\n",
    "from torchsummary import summary as torchsummary\n",
    "from torchinfo import summary as torchinfosummary\n",
    "\n",
    "########################## WORD EMBEDDING ##########################\n",
    "\n",
    "from torchtext.vocab import GloVe        # pretrained GloVe embeddings\n",
    "# from gensim.models import Word2Vec     # word2vec embeddings from corpus (commented out)\n",
    "\n",
    "########################## HUGGING FACE ##########################\n",
    "\n",
    "import transformers                      # transformers library core\n",
    "from transformers import (\n",
    "    GPT2Tokenizer, GPT2LMHeadModel,     # GPT-2 tokenizer and model\n",
    "    BertTokenizer, BertTokenizerFast, BertConfig, BertForMaskedLM,  # BERT components\n",
    "    XLNetTokenizer,                     # XLNet tokenizer\n",
    "    DistilBertForSequenceClassification, DistilBertTokenizer, AutoModelForSequenceClassification,\n",
    "    pipeline,                          # easy pipelines for inference\n",
    "    AutoTokenizer,                    # auto tokenizer loader\n",
    "    AutoModelForCausalLM, GPT2ForSequenceClassification,\n",
    "    DataCollatorForLanguageModeling, TrainingArguments, Trainer,  # training utilities\n",
    "    set_seed, GenerationConfig,\n",
    "    BertModel,                        # BERT base model\n",
    "    PreTrainedTokenizerBase\n",
    ")\n",
    "from datasets import DatasetDict         # HuggingFace dataset dictionaries\n",
    "\n",
    "######################### TRL & PEFT (TRAINING & PARAMETER EFFICIENT FINE-TUNING) ##########################\n",
    "\n",
    "from trl import (\n",
    "    SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM,\n",
    "    DPOConfig, DPOTrainer,\n",
    "    RewardTrainer, RewardConfig\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from torchmetrics import Accuracy        # metrics for evaluation\n",
    "\n",
    "########################## RAG ##########################\n",
    "\n",
    "from transformers import (\n",
    "    DPRQuestionEncoder, DPRQuestionEncoderTokenizer,\n",
    "    DPRContextEncoder, DPRContextEncoderTokenizer\n",
    ")\n",
    "import faiss                              # similarity search library\n",
    "\n",
    "########################## EVALUATION ##########################\n",
    "\n",
    "import evaluate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba1ef5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which device we are on: cpu\n"
     ]
    }
   ],
   "source": [
    "def accelerator(where = \"mps\"):\n",
    "    if where == \"mps\":\n",
    "        device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        print(\"Which device we are on: {}\".format(device))\n",
    "        return device\n",
    "    if where == \"cuda\":\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"Which device we are on: {}\".format(device))\n",
    "        return device\n",
    "    if where == \"cpu\":\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Which device we are on: {}\".format(device))\n",
    "        return device\n",
    "\n",
    "device = accelerator(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4697f770",
   "metadata": {},
   "source": [
    "# 0) CONCEPTS: RAG, FAISS, Prompt Engineering, LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfdd675",
   "metadata": {},
   "source": [
    "## Retrival Augmented Generation (RAG) and Facebook AI Similarity (FAISS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cbaf5e",
   "metadata": {},
   "source": [
    "**RAG** is a framework that helps optimize the output of LLMs without re-training the model, and by using ( for example) an internal database of a company. To do this, RAG comprises two main components:\n",
    "1. **The retriver**: the retriver combine:\n",
    "    1. **encoded prompt**: a high-dimensional vectorial representation of the prompt (which is translated in a vector using a **question encoder**, in the gray boc). The question encoder, at it ends, does an average;\n",
    "    2. **relevant context**: an 'internal' database (obtained using a **context encoder**, in the lightblue box, from internal documents of the company). ;\n",
    "\n",
    "   The retrival combine the relevant context and the prompt matching similar vectors in the embedded spaces. As vector similarities, we can use the dot product for the magnitude and the cosine similarity for the direction.\n",
    "\n",
    "2. **The generator**: using the data from the retriver, it answers to the user using a **decoder** (use `BartForConditionalGeneration` and `BartTokenizer`)\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*wMNhcGsiwDqVyxG1CFCj4w.jpeg\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4f84dd",
   "metadata": {},
   "source": [
    "For the context encoder use `DPRContextEncoderTokenizer` from `transformers`, which reads list of tuples, and `DPRContextEncoder` .\n",
    "\n",
    "For the question encoder use `DPRQuestionEncoderTokenizer` and `DPRQuestionEncoder`\n",
    "\n",
    "Library for compute the distance importing `faiss`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd2d08b",
   "metadata": {},
   "source": [
    "## In-context learning and prompt engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4e221c",
   "metadata": {},
   "source": [
    "**In-context learning** is a method of doing prompt engineering, and in particular we give to the model demonstration of the task provided.\n",
    "- Advantages: \n",
    "    1. Does not require fine-tuning --> reduce time\n",
    "    2. Improve performances\n",
    "- Disadvantages:\n",
    "    1. Limited to what fit in-context (what example can I realistically include in the prompt?)\n",
    "    2. Complex tasks may require gradient steps and adjustments based on gradients\n",
    "**Prompt Engineering**: prompts are divided in instructions and context (necessary background to do the task). PI is about how to ask a LLM questions in the best way possible. It is crucial to:\n",
    "1. One-shot prompt: give one example of i.e. translation before asking to translate a sentence;\n",
    "2. Few-shot prompts: giving some example of sentiment analysis before asking a new one;\n",
    "3. Chain of thought: give an example and break it into steps for the solution to be effective;\n",
    "4. Self consistency: 'when I was 6 my sister was half my age. Now I am 50, what age is my sister? Provide N independent calculations and explanations, then determine the most consistent result'. The model at the end choose the most frequent answer.\n",
    "\n",
    "Where test the prompts? \n",
    "1. Playground;\n",
    "2. LangChain: uses prompt templates, which include few shot examples.\n",
    "3. HuggingFace;\n",
    "4. IBM AI classroom;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7839902",
   "metadata": {},
   "source": [
    "## LangChain (chain of commands!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11f8bda",
   "metadata": {},
   "source": [
    "Components of LangChain:\n",
    "\n",
    "1. Language model: foundation of LLMs, using IBM, OpenAI, Google and Meta as primary language models\n",
    "2. Chat model: efficient conversation\n",
    "3. Chat message: efficient messages\n",
    "4. Prompt templates: translate user questions into clear instructions\n",
    "5. Output parser: transforms the output in suitable structured data\n",
    "\n",
    "\n",
    "We can use LangChain Documents to use RAG, and also to build applications (unifying chains, or sequence of calls!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd1c8cc",
   "metadata": {},
   "source": [
    "In recent years, the development of Large Language Models (LLMs) like GPT-3 and GPT-4 has revolutionized the field of natural language processing (NLP). These models are capable of performing a wide range of tasks, from generating coherent text to answering questions and summarizing information. Their effectiveness, however, is not without limitations. One significant constraint is the context window length, which affects how much information can be processed at once. LLMs operate within a fixed context window, measured in tokens, with GPT-3 having a limit of 4096 tokens and GPT-4 extending to 8192 tokens. When dealing with lengthy documents, attempting to input the entire text into the model's prompt can lead to truncation, where essential information is lost, and increased computational costs due to the processing of large inputs.\n",
    "\n",
    "These limitations become particularly pronounced when creating a retrieval-based question-answering (QA) assistant. The context length constraint restricts the ability to input all content into the prompt simultaneously, leading to potential loss of critical context and details. This necessitates the development of sophisticated strategies for selectively retrieving and processing relevant sections of the document. Techniques such as chunking the document into manageable parts, employing summarization methods, and using external retrieval systems are crucial to address these challenges. Understanding and mitigating these limitations are essential for designing effective QA systems that leverage the full potential of LLMs while navigating their inherent constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c463b4",
   "metadata": {},
   "source": [
    "# 1) RAG with HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc421ea3",
   "metadata": {},
   "source": [
    "For the RAG we will use:\n",
    "\n",
    "```\n",
    "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "context_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "```\n",
    "\n",
    "```\n",
    "question_encoder = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "```\n",
    "\n",
    "```\n",
    "decoder_tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "decoder = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd482538",
   "metadata": {},
   "source": [
    "## Dataset (context) preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b7ffe4",
   "metadata": {},
   "source": [
    "We will use the following text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a43db7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file downloaded\n",
      "1.\tCode of Conduct\n",
      "\n",
      "Our Code of Conduct outlines the fundamental principles and ethical standards that guide every member of our organization. We are committed to maintaining a workplace that is built on integrity, respect, and accountability.\n",
      "Integrity: We hold ourselves to the highest ethical standards. This means acting honestly and transparently in all our interactions, whether with colleagues, clients, or the broader community. We respect and protect sensitive information, and we avoid conflicts of interest.\n",
      "Respect: We embrace diversity and value each individual's contributions. Discrimination, harassment, or any form of disrespectful behavior is unacceptable. We create an inclusive environment where differences are celebrated and everyone is treated with dignity and courtesy.\n",
      "Accountability: We take responsibility for our actions and decisions. We follow all relevant laws and regulations, and we strive to continuously improve our practices. We report any potential violations of this code and support the investigation of such matters.\n",
      "Safety: We prioritize the safety of our employees, clients, and the communities we serve. We maintain a culture of safety, including reporting any unsafe conditions or practices.\n",
      "Environmental Responsibility: We are committed to minimizing our environmental footprint and promoting sustainable practices.\n",
      "Our Code of Conduct is not just a set of rules; it is the foundation of our organization's culture. We expect all employees to uphold these principles and serve as role models for others, ensuring we maintain our reputation for ethical conduct, integrity, and social responsibility.\n",
      "\n",
      "2.\tRecruitment Policy\n",
      "\n",
      "Our Recruitment Policy reflects our commitment to attracting, selecting, and onboarding the most qualified and diverse candidates to join our organization. We believe that the success of our company relies on the talents, skills, and dedication of our employees.\n",
      "Equal Opportunity: We are an equal opportunity employer and do not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or any other protected status. We actively promote diversity and inclusion.\n",
      "Transparency: We maintain transparency in our recruitment processes. All job vacancies are advertised internally and externally when appropriate. Job descriptions and requirements are clear and accurately represent the role.\n",
      "Selection Criteria: Our selection process is based on the qualifications, experience, and skills necessary for the position. Interviews and assessments are conducted objectively, and decisions are made without bias.\n",
      "Data Privacy: We are committed to protecting the privacy of candidates' personal information and adhere to all relevant data protection laws and regulations.\n",
      "Feedback: Candidates will receive timely and constructive feedback on their application and interview performance.\n",
      "Onboarding: New employees receive comprehensive onboarding to help them integrate into the organization effectively. This includes information on our culture, policies, and expectations.\n",
      "Employee Referrals: We encourage and appreciate employee referrals as they contribute to building a strong and engaged team.\n",
      "Our Recruitment Policy is a foundation for creating a diverse, inclusive, and talented workforce. It ensures that we attract and hire the best candidates who align with our company values and contribute to our continued success. We continuously review and update this policy to reflect evolving best practices in recruitment.\n",
      "\n",
      "3.\tInternet and Email Policy\n",
      "\n",
      "Our Internet and Email Policy is established to guide the responsible and secure use of these essential tools within our organization. We recognize their significance in daily business operations and the importance of adhering to principles that maintain security, productivity, and legal compliance.\n",
      "Acceptable Use: Company-provided internet and email services are primarily meant for job-related tasks. Limited personal use is allowed during non-work hours, provided it doesn't interfere with work responsibilities.\n",
      "Security: Safeguard your login credentials, avoiding the sharing of passwords. Exercise caution with email attachments and links from unknown sources. Promptly report any unusual online activity or potential security breaches.\n",
      "Confidentiality: Reserve email for the transmission of confidential information, trade secrets, and sensitive customer data only when encryption is applied. Exercise discretion when discussing company matters on public forums or social media.\n",
      "Harassment and Inappropriate Content: Internet and email usage must not involve harassment, discrimination, or the distribution of offensive or inappropriate content. Show respect and sensitivity to others in all online communications.\n",
      "Compliance: Ensure compliance with all relevant laws and regulations regarding internet and email usage, including those related to copyright and data protection.\n",
      "Monitoring: The company retains the right to monitor internet and email usage for security and compliance purposes.\n",
      "Consequences: Policy violations may lead to disciplinary measures, including potential termination.\n",
      "Our Internet and Email Policy aims to promote safe, responsible usage of digital communication tools that align with our values and legal obligations. Each employee is expected to understand and follow this policy. Regular reviews ensure its alignment with evolving technology and security standards.\n",
      "\n",
      "4.\tMobile Phone Policy\n",
      "\n",
      "The Mobile Phone Policy sets forth the standards and expectations governing the appropriate and responsible usage of mobile devices in the organization. The purpose of this policy is to ensure that employees utilize mobile phones in a manner consistent with company values and legal compliance.\n",
      "Acceptable Use: Mobile devices are primarily intended for work-related tasks. Limited personal usage is allowed, provided it does not disrupt work obligations.\n",
      "Security: Safeguard your mobile device and access credentials. Exercise caution when downloading apps or clicking links from unfamiliar sources. Promptly report security concerns or suspicious activities related to your mobile device.\n",
      "Confidentiality: Avoid transmitting sensitive company information via unsecured messaging apps or emails. Be discreet when discussing company matters in public spaces.\n",
      "Cost Management: Keep personal phone usage separate from company accounts and reimburse the company for any personal charges on company-issued phones.\n",
      "Compliance: Adhere to all pertinent laws and regulations concerning mobile phone usage, including those related to data protection and privacy.\n",
      "Lost or Stolen Devices: Immediately report any lost or stolen mobile devices to the IT department or your supervisor.\n",
      "Consequences: Non-compliance with this policy may lead to disciplinary actions, including the potential loss of mobile phone privileges.\n",
      "The Mobile Phone Policy is aimed at promoting the responsible and secure use of mobile devices in line with legal and ethical standards. Every employee is expected to comprehend and abide by these guidelines. Regular reviews of the policy ensure its ongoing alignment with evolving technology and security best practices.\n",
      "\n",
      "5.\tSmoking Policy\n",
      "\n",
      "Policy Purpose: The Smoking Policy has been established to provide clear guidance and expectations concerning smoking on company premises. This policy is in place to ensure a safe and healthy environment for all employees, visitors, and the general public.\n",
      "Designated Smoking Areas: Smoking is only permitted in designated smoking areas, as marked by appropriate signage. These areas have been chosen to minimize exposure to secondhand smoke and to maintain the overall cleanliness of the premises.\n",
      "Smoking Restrictions: Smoking inside company buildings, offices, meeting rooms, and other enclosed spaces is strictly prohibited. This includes electronic cigarettes and vaping devices.\n",
      "Compliance with Applicable Laws: All employees and visitors must adhere to relevant federal, state, and local smoking laws and regulations.\n",
      "Disposal of Smoking Materials: Properly dispose of cigarette butts and related materials in designated receptacles. Littering on company premises is prohibited.\n",
      "No Smoking in Company Vehicles: Smoking is not permitted in company vehicles, whether they are owned or leased, to maintain the condition and cleanliness of these vehicles.\n",
      "Enforcement and Consequences: All employees and visitors are expected to adhere to this policy. Non-compliance may lead to appropriate disciplinary action, which could include fines, or, in the case of employees, possible termination of employment.\n",
      "Review of Policy: This policy will be reviewed periodically to ensure its alignment with evolving legal requirements and best practices for maintaining a healthy and safe workplace.\n",
      "We appreciate your cooperation in maintaining a smoke-free and safe environment for all.\n",
      "\n",
      "6.\tDrug and Alcohol Policy\n",
      "\n",
      "Policy Objective: The Drug and Alcohol Policy is established to establish clear expectations and guidelines for the responsible use of drugs and alcohol within the organization. This policy aims to maintain a safe, healthy, and productive workplace.\n",
      "Prohibited Substances: The use, possession, distribution, or sale of illegal drugs or unauthorized controlled substances is strictly prohibited on company premises or during work-related activities. This includes the misuse of prescription drugs.\n",
      "Alcohol Consumption: The consumption of alcoholic beverages is not allowed during work hours, on company property, or while performing company-related duties. Exception may be made for company-sanctioned events.\n",
      "Impairment: Employees are expected to perform their job duties without impairment from drugs or alcohol. The use of substances that could impair job performance or pose a safety risk is prohibited.\n",
      "Testing and Searches: The organization reserves the right to conduct drug and alcohol testing as per applicable laws and regulations. Employees may be subject to testing in cases of reasonable suspicion, post-accident, or as part of routine workplace safety measures.\n",
      "Reporting: Employees should report any concerns related to drug or alcohol misuse by themselves or their colleagues, as well as safety concerns arising from such misuse.\n",
      "Treatment and Assistance: Employees with substance abuse issues are encouraged to seek help. The organization is committed to providing support, resources, and information to assist those seeking treatment.\n",
      "Consequences: Violation of this policy may result in disciplinary actions, up to and including termination of employment. Legal action may also be pursued when necessary.\n",
      "Policy Review: This policy will undergo periodic review to ensure its continued relevance and compliance with evolving legal requirements and best practices for a safe and productive work environment.\n",
      "Your adherence to this policy is appreciated as it helps to maintain a safe and drug-free workplace for all.\n",
      "\n",
      "7.\tHealth and Safety Policy\n",
      "\n",
      "Our commitment to health and safety is paramount. We prioritize the well-being of our employees, customers, and the public. We diligently comply with all relevant health and safety laws and regulations. Our objective is to maintain a workplace free from hazards, preventing accidents, injuries, and illnesses. Every individual within our organization is responsible for upholding these standards. We regularly assess and improve our safety measures, provide adequate training, and encourage open communication regarding safety concerns. Through collective dedication, we aim to ensure a safe, healthy, and secure environment for all. Your cooperation is essential in achieving this common goal.\n",
      "\n",
      "8.\tAnti-discrimination and Harassment Policy\n",
      "\n",
      "The Anti-Discrimination and Harassment Policy is a testament to the commitment of this organization in fostering a workplace that is free from discrimination, harassment, and any form of unlawful bias. This policy applies to every individual within the organization, including employees, contractors, visitors, and clients.\n",
      "Non-Discrimination: This organization strictly prohibits discrimination based on race, color, religion, gender, national origin, age, disability, sexual orientation, or any other legally protected characteristic in all aspects of employment, including recruitment, hiring, compensation, benefits, promotions, and terminations.\n",
      "Harassment: Harassment in any form, whether based on the aforementioned characteristics or any other protected status, is unacceptable. This encompasses unwelcome advances, offensive jokes, slurs, and other verbal or physical conduct that creates a hostile or intimidating work environment.\n",
      "Reporting: Individuals who experience or witness any form of discrimination or harassment are encouraged to promptly report the incident to their supervisor, manager, or the designated HR representative. The organization is committed to a timely and confidential investigation of such complaints.\n",
      "Consequences: Violation of this policy may result in disciplinary action, including termination of employment. The organization is committed to taking appropriate action against any individual found to be in violation of this policy.\n",
      "Review and Update: This policy is subject to regular review and update to remain aligned with evolving legal requirements and best practices in preventing discrimination and harassment. This organization considers it a collective responsibility to ensure a workplace free from discrimination and harassment, and it is essential that every individual within the organization plays their part in upholding these principles.\n",
      "\n",
      "9.\tDiscipline and Termination Policy\n",
      "\n",
      "The Discipline and Termination Policy underscores the organization's commitment to maintaining a productive, ethical, and respectful work environment. This policy applies to all personnel, including employees, contractors, and temporary staff.\n",
      "Performance and Conduct Expectations: Employees are expected to meet performance standards and adhere to conduct guidelines. The organization will provide clear expectations, feedback, and opportunities for improvement when performance or conduct issues arise.\n",
      "Disciplinary Actions: When necessary, disciplinary actions will be taken, which may include verbal warnings, written warnings, suspension, or other appropriate measures. Disciplinary actions are designed to address issues constructively and maintain performance standards.\n",
      "Termination: In situations where an employee's performance or conduct issues persist, the organization may resort to termination. Termination may also occur for reasons such as redundancy, violation of policies, or restructuring.\n",
      "Termination Procedure: The organization will follow appropriate procedures, ensuring fairness and adherence to legal requirements during the termination process. Employees may be eligible for notice periods, severance pay, or other benefits as per employment agreements and applicable laws.\n",
      "Exit Process: The organization will conduct an exit process to ensure a smooth transition for departing employees, including the return of company property, final pay, and cancellation of access and benefits.\n",
      "This policy serves as a framework for handling discipline and termination. The organization recognizes the importance of fairness and consistency in these processes, and decisions will be made after careful consideration. Every employee is expected to understand and adhere to this policy, contributing to a respectful and productive workplace. Regular reviews will ensure its alignment with evolving legal requirements and best practices.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename = 'companyPolicies.txt'\n",
    "url = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/6JDbUb_L3egv_eOkouY71A.txt'\n",
    "\n",
    "# Use wget to download the file\n",
    "wget.download(url, out = filename)\n",
    "print('file downloaded')\n",
    "\n",
    "with open(\"companyPolicies.txt\", 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432cbdce",
   "metadata": {},
   "source": [
    "We split it into paragraphs, with `text.split('\\n')`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fed0078f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8.\\tAnti-discrimination and Harassment Policy'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_and_split_text(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    # Split the text into paragraphs (simple split by newline characters)\n",
    "    paragraphs = text.split('\\n')\n",
    "    # Filter out any empty paragraphs or undesired entries\n",
    "    paragraphs = [para.strip() for para in paragraphs if len(para.strip()) > 0]\n",
    "    return paragraphs\n",
    "\n",
    "# Read the text file and split it into paragraphs\n",
    "\n",
    "paragraphs = read_and_split_text('companyPolicies.txt')\n",
    "random.shuffle(paragraphs) #shuffling samples so that the samples are not ordered based on the category they belong to\n",
    "paragraphs[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8111d2",
   "metadata": {},
   "source": [
    "## Define the pre-trained context encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cee581",
   "metadata": {},
   "source": [
    "Let's use the Dense Passage Retriever (DPR) model, specifically the context encoder, to convert your preprocessed text data into dense vector embeddings. These embeddings capture the semantic meanings of the texts, enabling effective similarity-based retrieval. DPR models, such as the the DPRContextEncoder and DPRContextEncoderTokenizer, are built on the BERT architecture but specialize in dense passage retrieval. They differ from BERT in their training, which focuses on contrastive learning for retrieving relevant passages, while BERT is more general-purpose, handling various NLP tasks. Passages are:\n",
    "1. tokenize\n",
    "2. encode\n",
    "3. aggregate in a single vector\n",
    "\n",
    "At the end of this section, the function `encode_contexts` does everything together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514e0fc7",
   "metadata": {},
   "source": [
    "Ignore the warnings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef2ba724",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/Trans_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.weight', 'ctx_encoder.bert_model.pooler.dense.bias']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "# the mebdding dimension is 768\n",
    "context_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272edafa",
   "metadata": {},
   "source": [
    "Usage example of the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c47fdd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\t\tResult of 'context_tokenizer': \n",
      "\n",
      " {'input_ids': tensor([[ 101, 2129, 2024, 2017, 1029,  102, 1045, 2572, 2986, 1010, 1045, 2903,\n",
      "         1012,  102],\n",
      "        [ 101, 2054, 1005, 1055, 2039, 1029,  102, 2025, 2172, 1012,  102,    0,\n",
      "            0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])} \n",
      "\n",
      " ----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t\t\t\t\tResult of 'context_tokenizer.convert_ids_to_tokens'\n",
      "\n",
      "\n",
      "['[CLS]', 'how', 'are', 'you', '?', '[SEP]', 'i', 'am', 'fine', ',', 'i', 'believe', '.', '[SEP]']\n",
      "['[CLS]', 'what', \"'\", 's', 'up', '?', '[SEP]', 'not', 'much', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "# format of text must be [(\"sentence A\", \"sentence B\"),(,),(,),...,(,)]\n",
    "text_list = [(\"How are you?\", \"I am fine, I believe.\"), (\"What's up?\", \"Not much.\")]\n",
    "\n",
    "tokens_info = context_tokenizer(text_list, return_tensors = 'pt', padding = True, truncation = True, max_length = 256)\n",
    "\n",
    "print(f\"\\t\\t\\t\\t\\t\\tResult of 'context_tokenizer': \\n\\n {tokens_info} \\n\\n\", \"-\"*160)\n",
    "\n",
    "# the size of the following three is torch.Size([len(text), max lenght of sentence A+B])\n",
    "tokens_info['input_ids'].shape \n",
    "tokens_info['token_type_ids'].shape # these are the standard BERT-like segment IDs of each ()\n",
    "tokens_info['attention_mask'].shape # 1 for actual tokens, 0 for the paddings added during padding = True\n",
    "\n",
    "print(f\"\\t\\t\\t\\t\\tResult of 'context_tokenizer.convert_ids_to_tokens'\\n\\n\")\n",
    "for s in tokens_info['input_ids']: #get the original text\n",
    "   print(f\"{context_tokenizer.convert_ids_to_tokens(s)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344373f7",
   "metadata": {},
   "source": [
    "Usage example of the encoder:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbffbe4",
   "metadata": {},
   "source": [
    "`context_encoder(**tokens_info)`is equivalent to \n",
    "```\n",
    "context_encoder(tokens_info['input_ids'],\n",
    "                tokens_info['token_type_ids'],\n",
    "                tokens_info['attention_mask'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1d39c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\tEmbedding tensors with shape torch.Size([2, 768]): \n",
      "\n",
      " tensor([[ 0.1607,  0.7112, -0.0994,  ..., -0.3211,  0.6649, -0.0329],\n",
      "        [ 0.6606,  0.3294,  0.3890,  ..., -0.0723,  0.3644, -0.1266]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      " \n"
     ]
    }
   ],
   "source": [
    "context_encoder(**tokens_info) #embedding\n",
    "\n",
    "print(f\"\\t\\t\\t\\t\\tEmbedding tensors with shape {context_encoder(**tokens_info).pooler_output.shape}: \\n\\n {context_encoder(**tokens_info).pooler_output} \\n \") # embedding PyTorch tensor without other informations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfb7f1c",
   "metadata": {},
   "source": [
    "Usage example of the preparation of the context encoding. Notice that if we do not write `[segments]` but only `segments`, the resulting shape would be [2,768] instead of the more correct [1,768]. This is because Hugging Face interprets `(,)` as two distinc samples, instead of the correct input-context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfd1037c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.16070484,  0.711192  , -0.0993585 , ..., -0.3211333 ,\n",
       "         0.66492313, -0.03289769],\n",
       "       [ 0.6606474 ,  0.32937145,  0.389035  , ..., -0.07225327,\n",
       "         0.36436284, -0.1265702 ]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list = [(\"How are you?\", \"I am fine, I believe.\"), (\"What's up?\", \"Not much.\")]\n",
    "embeddings = []\n",
    "for segments in text_list:\n",
    "\n",
    "    input_of_encoder = context_tokenizer([segments], return_tensors = 'pt', padding = True, truncation = True, max_length = 256)\n",
    "    embedding_tensor = context_encoder(**input_of_encoder).pooler_output # embedding_tensor.shape is torch.Size([1, 768])\n",
    "    embeddings.append(embedding_tensor)\n",
    "\n",
    "\n",
    "torch.cat(embeddings).detach().numpy() #this is the aggregation, with shape torch.Size([2, 768])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264b423e",
   "metadata": {},
   "source": [
    "The previous cell but converted in a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa861c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_contexts(text_list):\n",
    "    embeddings = []\n",
    "    for segments in text_list:\n",
    "        input_of_encoder = context_tokenizer([segments], return_tensors = 'pt', padding = True, truncation = True, max_length = 256)\n",
    "        embedding_tensor = context_encoder(**input_of_encoder).pooler_output\n",
    "        embeddings.append(embedding_tensor) \n",
    "    return torch.cat(embeddings).detach().numpy() #aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e9ad13",
   "metadata": {},
   "source": [
    "Usage example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9568f375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.16070484,  0.711192  , -0.0993585 , ..., -0.3211333 ,\n",
       "         0.66492313, -0.03289769],\n",
       "       [ 0.6606474 ,  0.32937145,  0.389035  , ..., -0.07225327,\n",
       "         0.36436284, -0.1265702 ]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list = [(\"How are you?\", \"I am fine, I believe.\"), (\"What's up?\", \"Not much.\")]\n",
    "encode_contexts(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc699538",
   "metadata": {},
   "source": [
    "So for the paragraphs in our original text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f0403df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76, 768)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this takes 45 sec \n",
    "\n",
    "context_embeddings = encode_contexts(paragraphs)\n",
    "context_embeddings.shape # (len(paragraphs) = 76, 768)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd0c4b2",
   "metadata": {},
   "source": [
    "## FAISS index / vector space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa44613",
   "metadata": {},
   "source": [
    "`vector_space = faiss.IndexFlatL2(d)` initializes a d-dimensional vector space, so that after we can add the vectorial representation that we have obtained with the `encode_contexts` function in the previous step. After we have filled out our `vector_space`, we can perform a similarity search (faiss uses squared L2 distance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95a5d3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 768  # This should match the dimension of the encoder dimension\n",
    "vector_space = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "\n",
    "context_embeddings_np = np.array(context_embeddings).astype('float32')\n",
    "context_embeddings_np.shape # (len(paragraphs) = 76, 768)\n",
    "vector_space.add(context_embeddings_np) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abb0770",
   "metadata": {},
   "source": [
    "If we want, we can also compute the maximum distance among the embedded context vectors. This can be useful to normalize the distances in the retrieval process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbdc72d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max squared L2 distance: 209.3562\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "dist_matrix = squareform(pdist(context_embeddings_np, metric = 'euclidean'))\n",
    "\n",
    "max_distance = np.max(dist_matrix)\n",
    "max_distance_squared = max_distance ** 2\n",
    "\n",
    "print(f\"Max squared L2 distance: {max_distance_squared:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6e17f1",
   "metadata": {},
   "source": [
    "## Define the pre-trained  question encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cf66a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.weight', 'question_encoder.bert_model.pooler.dense.bias']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "question_encoder = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b825ad7",
   "metadata": {},
   "source": [
    "## Retrival process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6a0a01",
   "metadata": {},
   "source": [
    "Now we have all the encoders and the distance metric (faiss), so now we can retrive. First, process an example query by converting the raw text question into a format that the DPR question encoder can understand and then encode it into a dense vector. Using the encoded question, search your prebuilt FAISS index to find the most relevant contexts. This step showcases the practical use of the FAISS index in retrieving information based on query similarity.\n",
    "\n",
    "After conducting the search for relevant contexts based on the question embedding, the output consists of two key components:\n",
    "\n",
    "- **D (Distances)**: This array contains the distances between the query embedding and the retrieved document embeddings. The distances measure the similarity between the query and each document, where lower distances indicate higher relevance. These values help determine how closely each retrieved context matches the query.\n",
    "\n",
    "- **I (Indices)**: This array holds the indices of the paragraphs within the `paragraphs` array that have been identified as the most relevant to the query. These indices correspond to the positions of the paragraphs in the original data array, allowing for easy retrieval of the actual text content.\n",
    "\n",
    "The combination of `D` and `I` provides both a quantitative measure of relevance and the specific content that is most relevant, enabling a comprehensive response to the user's query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae26be6e",
   "metadata": {},
   "source": [
    "**Source of confusion**: even if in the following cell there is no direct mention to the context, it is already encapsulated in the faiss, which is called with `vector_space.search`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3aeb5e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distances: [72.76531  74.7162   84.388115 88.36438  90.287125]\n",
      "Normalized distances (percentages): [34.76 35.69 40.31 42.21 43.13]%\n",
      "Indices: [45 21  9 30 18]\n"
     ]
    }
   ],
   "source": [
    "question = 'Drug and Alcohol Policy'\n",
    "question_inputs = question_tokenizer(question, return_tensors = 'pt')\n",
    "question_embedding = question_encoder(**question_inputs).pooler_output\n",
    "question_embedding_np = question_embedding.detach().numpy()\n",
    "\n",
    "# Search on the vector_space\n",
    "number_of_top_results = 5\n",
    "D, I = vector_space.search(question_embedding_np, k = number_of_top_results)  # Retrieve top 5 relevant contexts\n",
    "print(f\"Distances: {D[0]}\")\n",
    "print(f\"Normalized distances (percentages): {np.round(D[0]/max_distance_squared*100,2)}%\")\n",
    "print(f\"Indices: {I[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9be0a691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\t\t\tTop 5 relevant contexts:\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Result number 1 with distance  34.76%: \n",
      " 6.\tDrug and Alcohol Policy\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Result number 2 with distance  35.69%: \n",
      " Policy Objective: The Drug and Alcohol Policy is established to establish clear expectations and guidelines for the responsible use of drugs and alcohol within the organization. This policy aims to maintain a safe, healthy, and productive workplace.\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Result number 3 with distance  40.31%: \n",
      " Testing and Searches: The organization reserves the right to conduct drug and alcohol testing as per applicable laws and regulations. Employees may be subject to testing in cases of reasonable suspicion, post-accident, or as part of routine workplace safety measures.\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Result number 4 with distance  42.21%: \n",
      " 9.\tDiscipline and Termination Policy\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Result number 5 with distance  43.13%: \n",
      " Monitoring: The company retains the right to monitor internet and email usage for security and compliance purposes.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\t\\t\\t\\t\\t\\t\\tTop {number_of_top_results} relevant contexts:\")\n",
    "for i, idx in enumerate(I[0]):\n",
    "    print(\"-\"*160)\n",
    "    print(f\"Result number {i+1} with distance  {np.round(D[0][i]/max_distance_squared*100,2)}%: \\n {paragraphs[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faeac92",
   "metadata": {},
   "source": [
    "Let's convert the above to a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bd676d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_relevant_contexts(question, question_tokenizer, question_encoder, vector_space, k = 5, display = False):\n",
    "\n",
    "    question_inputs = question_tokenizer(question, return_tensors='pt')\n",
    "    question_embedding = question_encoder(**question_inputs).pooler_output\n",
    "    question_embedding_np = question_embedding.detach().numpy()\n",
    "\n",
    "    # Search the index to retrieve top k relevant contexts\n",
    "    D, I = vector_space.search(question_embedding_np, k)\n",
    "    if display == True:\n",
    "        print(f\"\\t\\t\\t\\t\\t\\t\\tTop {number_of_top_results} relevant contexts:\")\n",
    "        for i, idx in enumerate(I[0]):\n",
    "            print(\"-\"*160)\n",
    "            print(f\"Result number {i+1} with distance  {np.round(D[0][i]/max_distance_squared*100,2)}%: \\n {paragraphs[idx]}\")\n",
    "\n",
    "    return D, I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3208422f",
   "metadata": {},
   "source": [
    "Usage example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb78bd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\t\t\tTop 5 relevant contexts:\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Result number 1 with distance  34.76%: \n",
      " 6.\tDrug and Alcohol Policy\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Result number 2 with distance  35.69%: \n",
      " Policy Objective: The Drug and Alcohol Policy is established to establish clear expectations and guidelines for the responsible use of drugs and alcohol within the organization. This policy aims to maintain a safe, healthy, and productive workplace.\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Result number 3 with distance  40.31%: \n",
      " Testing and Searches: The organization reserves the right to conduct drug and alcohol testing as per applicable laws and regulations. Employees may be subject to testing in cases of reasonable suspicion, post-accident, or as part of routine workplace safety measures.\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Result number 4 with distance  42.21%: \n",
      " 9.\tDiscipline and Termination Policy\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Result number 5 with distance  43.13%: \n",
      " Monitoring: The company retains the right to monitor internet and email usage for security and compliance purposes.\n"
     ]
    }
   ],
   "source": [
    "question = 'Drug and Alcohol Policy'\n",
    "number_of_top_results = 5\n",
    "D, I = search_relevant_contexts(question, question_tokenizer, question_encoder, vector_space, k = number_of_top_results, display = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6445b0",
   "metadata": {},
   "source": [
    "## From retrival to final answer: decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76467f66",
   "metadata": {},
   "source": [
    "We will use GPT-2 but this is not the best: GPT-2 is trained to next-token prediction and not for Q-A based on context. Better choices could be \"google/flan-t5-base\", \"facebook/bart-large\", \"allenai/t5-small-squad2\" or \"deepset/tinyroberta-squad2\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77631f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "decoder = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "decoder_tokenizer.pad_token = decoder_tokenizer.eos_token\n",
    "decoder.config.pad_token_id = decoder_tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f159ad09",
   "metadata": {},
   "source": [
    "Now we build the final answer using the decoder. The function `generate_answer` will receive the question (the same question received by `search_relevant_contexts`) and the contexts coming from `search_relevant_contexts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69d8fa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the one by IBM. It is not very precise, because it concatenates the input question and the top contexts\n",
    "# It is faster and simple, but less precise\n",
    "\n",
    "def generate_answer(question, contexts): \n",
    "    \n",
    "    input_text = question + ' ' + ' '.join(contexts) # Concatenate the retrieved contexts to form the input to GPT2\n",
    "    inputs = decoder_tokenizer(input_text, return_tensors = 'pt', max_length = 1024, truncation = True)\n",
    "\n",
    "    summary_ids = decoder.generate(\n",
    "                            inputs['input_ids'],\n",
    "                            attention_mask = inputs['attention_mask'],  #  Added!\n",
    "                            max_new_tokens = 50, min_length = 40,\n",
    "                            length_penalty = 2.0,\n",
    "                            num_beams = 4,\n",
    "                            early_stopping = True,\n",
    "                            pad_token_id = decoder_tokenizer.pad_token_id)\n",
    "\n",
    "    return decoder_tokenizer.decode(summary_ids[0], skip_special_tokens = True)\n",
    "\n",
    "# More precise function, but slower\n",
    "\n",
    "def my_generate_answer(question, contexts):\n",
    "    candidates = []\n",
    "    for context in contexts:\n",
    "        input_text = question + '\\n' + context\n",
    "        inputs = decoder_tokenizer(input_text, return_tensors = 'pt', max_length = 512, truncation = True)\n",
    "        summary_ids = decoder.generate(inputs['input_ids'],\n",
    "                            attention_mask = inputs['attention_mask'],  #  Added!\n",
    "                            max_new_tokens = 50, min_length = 40,\n",
    "                            length_penalty = 2.0,\n",
    "                            num_beams = 4,\n",
    "                            early_stopping = True,\n",
    "                            pad_token_id = decoder_tokenizer.pad_token_id)\n",
    "        response = decoder_tokenizer.decode(summary_ids[0], skip_special_tokens = True)\n",
    "        candidates.append(response)\n",
    "    \n",
    "    # puoi implementare una metrica, oppure restituire tutti i risultati\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61efa42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/Trans_env/lib/python3.10/site-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\t\t\tgenerate_answer Results:\n",
      "\n",
      "what is mobile policy? 4.\tMobile Phone Policy The Mobile Phone Policy sets forth the standards and expectations governing the appropriate and responsible usage of mobile devices in the organization. The purpose of this policy is to ensure that employees utilize mobile phones in a manner consistent with company values and legal compliance. Monitoring: The company retains the right to monitor internet and email usage for security and compliance purposes. Acceptable Use: Mobile devices are primarily intended for work-related tasks. Limited personal usage is allowed, provided it does not disrupt work obligations. The Mobile Phone Policy is aimed at promoting the responsible and secure use of mobile devices in line with legal and ethical standards. Every employee is expected to comprehend and abide by these guidelines. Regular reviews of the policy ensure its ongoing alignment with evolving technology and security best practices.\n",
      "\n",
      "The Mobile Phone Policy sets forth the standards and expectations governing the appropriate and responsible usage of mobile devices in the organization. The purpose of this policy is to ensure that employees utilize mobile phones in a manner consistent with company values and legal compliance. Monitoring\n",
      " ----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t\t\t\t\t\t\tmy_generate_answer Results:\n",
      "\n",
      "\n",
      "Result 0: what is mobile policy?\n",
      "4.\tMobile Phone Policy\n",
      "4.ailability of Mobile Policy\n",
      "4.1. Mobile Policy\n",
      "4.1a. Mobile Policy\n",
      "4.1b. Mobile Policy\n",
      "4.1c. Mobile Policy\n",
      "4.1d. Mobile Policy\n",
      "4.1\n",
      "Result 1: what is mobile policy?\n",
      "The Mobile Phone Policy sets forth the standards and expectations governing the appropriate and responsible usage of mobile devices in the organization. The purpose of this policy is to ensure that employees utilize mobile phones in a manner consistent with company values and legal compliance.\n",
      "The Mobile Phone Policy is designed to ensure that employees use mobile phones in a manner consistent with company values and legal compliance.\n",
      "The Mobile Phone Policy is designed to ensure that employees use mobile phones in a manner consistent with company values and legal compliance.\n",
      "Result 2: what is mobile policy?\n",
      "Monitoring: The company retains the right to monitor internet and email usage for security and compliance purposes. The company also retains the right to monitor the internet and email usage for security and compliance purposes. The company also retains the right to monitor the internet and email usage for security and compliance purposes. The company also retains the right to monitor the internet and email\n",
      "Result 3: what is mobile policy?\n",
      "Acceptable Use: Mobile devices are primarily intended for work-related tasks. Limited personal usage is allowed, provided it does not disrupt work obligations.\n",
      "Acceptable Use: Mobile devices are primarily intended for work-related tasks. Limited personal usage is allowed, provided it does not disrupt work obligations.\n",
      "Acceptable Use: Mobile devices are primarily intended for work-related tasks. Limited personal usage is\n",
      "Result 4: what is mobile policy?\n",
      "The Mobile Phone Policy is aimed at promoting the responsible and secure use of mobile devices in line with legal and ethical standards. Every employee is expected to comprehend and abide by these guidelines. Regular reviews of the policy ensure its ongoing alignment with evolving technology and security best practices.\n",
      "The Mobile Phone Policy is designed to promote the responsible and secure use of mobile devices in line with legal and ethical standards. Every employee is expected to comprehend and abide by these guidelines. Regular reviews of the policy ensure its ongoing alignment with evolving technology and\n"
     ]
    }
   ],
   "source": [
    "# it takes 1m\n",
    "question = \"what is mobile policy?\"\n",
    "\n",
    "_, I = search_relevant_contexts(question, question_tokenizer, question_encoder, vector_space, k = 5, display = False)\n",
    "top_contexts = [paragraphs[idx] for idx in I[0]] \n",
    "\n",
    "answer = generate_answer(question, top_contexts)\n",
    "my_answers = my_generate_answer(question, top_contexts)\n",
    "print(f\"\\t\\t\\t\\t\\t\\t\\tgenerate_answer Results:\\n\\n{answer}\\n\",\"-\"*160)\n",
    "print(f\"\\t\\t\\t\\t\\t\\t\\tmy_generate_answer Results:\\n\\n\")\n",
    "for i, my_answer in enumerate(my_answers):\n",
    "    print(f\"Result {i}: {my_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d969af",
   "metadata": {},
   "source": [
    "# 2) (simplified) RAG with PyTorch \n",
    "\n",
    "Similarity-based retrieval of static answers using BERT mean embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5491d241",
   "metadata": {},
   "source": [
    "In this chapter we want to understand if a song is ok for children, and for doing this we use a similarity-based retrival. We have:\n",
    "1. pre-defined set of questions `song_questions`;\n",
    "2. lyrics of a song as `sesame_street` and `my_shoe_lyrics`;\n",
    "3. list of predefined answers, **not generated**, which is `yes_responses`.\n",
    "\n",
    "**Very important**: notice the Q and A are associated, this is the reason for which the answers embedding will not be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace46b0f",
   "metadata": {},
   "source": [
    "## Dataset: questions and answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092be837",
   "metadata": {},
   "source": [
    "Questions and answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e168751",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_questions = [\n",
    "    \"Does this song contain any violent themes, such as references to guns, killing, or physical aggression? Example: Does the song describe or promote physical violence, like fighting or shootings?\",\n",
    "    \"Are there any explicit lyrics or bad words used in this song that might be considered offensive or inappropriate? Example: Does the song use language commonly recognized as profanity or derogatory terms?\",\n",
    "    \"Is the overall content of this song suitable for children, considering its themes, language, and messages? Example: Are there elements in the song that could be deemed too mature or unsuitable for young listeners?\",\n",
    "    \"Does this song explicitly mention weapons, such as guns, knives, or other similar items? Example: Are specific types of weapons described or glorified in the lyrics?\",\n",
    "    \"Are the messages conveyed in this song positive and uplifting for children? Example: Does the song promote values like kindness, friendship, and positivity?\",\n",
    "    \"Does this song include any sexual content, references to sexual behavior, or suggestive language? Example: Are there lyrics that explicitly or implicitly discuss sexual themes or experiences?\",\n",
    "    \"Does this song offer any educational value, such as teaching the alphabet, basic math, or other learning content? Example: Are there educational segments in the song that could help children learn fundamental skills like the ABCs or counting?\",\n",
    "    \"Does this song promote emotional resilience and social skills among children? Example: Does the song include themes of overcoming challenges or building friendships?\"\n",
    "]\n",
    "\n",
    "yes_responses = [\n",
    "    \"Yes, this song contains violent themes, including references to guns, killing, or physical aggression, and is not suitable for children.\",\n",
    "    \"Yes, this song includes explicit lyrics or bad words that might be considered offensive or inappropriate for young audiences.\",\n",
    "    \"No, the overall content of this song is not suitable for children as it includes themes, language, and messages that are too mature or unsuitable for young listeners.\",\n",
    "    \"Yes, this song explicitly mentions weapons, such as guns and knives, which could be disturbing or inappropriate for childrens entertainment.\",\n",
    "    \"Yes, the messages conveyed in this song are positive and uplifting, promoting values like kindness, friendship, and positivity, beneficial for children.\",\n",
    "    \"Yes, this song includes sexual content and references to sexual behavior or suggestive language, which are inappropriate for a child-friendly environment.\",\n",
    "    \"Yes, this song offers significant educational value, including segments that teach the alphabet, basic math, and other learning content, making it both fun and educational for children.\",\n",
    "    \"Yes, this song promotes emotional resilience and social skills, incorporating themes about overcoming challenges and building friendships, which are essential for children's development.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6208f179",
   "metadata": {},
   "source": [
    "Song lyrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9c46086",
   "metadata": {},
   "outputs": [],
   "source": [
    "sesame_street_lyrics = \"\"\"\n",
    "Sunny day\n",
    "Sweepin' the clouds away\n",
    "On my way to where the air is sweet\n",
    "Can you tell me how to get\n",
    "How to get to Sesame Street?\n",
    "\n",
    "Come and play\n",
    "Everything's A-okay\n",
    "Friendly neighbors there\n",
    "That's where we meet\n",
    "Can you tell me how to get\n",
    "How to get to Sesame Street?\n",
    "\n",
    "It's a magic carpet ride\n",
    "Every door will open wide\n",
    "To happy people like you\n",
    "Happy people like\n",
    "What a beautiful\n",
    "\n",
    "Sunny day\n",
    "Sweepin' the clouds away\n",
    "On my way to where the air is sweet\n",
    "Can you tell me how to get\n",
    "How to get to Sesame Street?\n",
    "How to get to Sesame Street?\n",
    "How to get to Sesame Street?\n",
    "How to get to Sesame Street?\n",
    "How to get to Sesame Street?\n",
    "\"\"\"\n",
    "\n",
    "my_shoe_lyrics = \"\"\"Barney is a dinosaur from our imagination\n",
    "And when he's tall\n",
    "He's what we call a dinosaur sensation\n",
    "Barney's friends are big and small\n",
    "They come from lots of places\n",
    "After school they meet to play\n",
    "And sing with happy faces\n",
    "Barney shows us lots of things\n",
    "Like how to play pretend\n",
    "ABC's, and 123's\n",
    "And how to be a friend\n",
    "Barney comes to play with us\n",
    "Whenever we may need him\n",
    "Barney can be your friend too\n",
    "If you just make-believe him!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "196c26cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_song(song):\n",
    "    # Remove line breaks from the song\n",
    "    song_new = re.sub(r'[\\n]', ' ', song)\n",
    "    \n",
    "    # Remove single quotes from the song\n",
    "    processed_song = [song_new.replace(\"\\'\", \"\")]\n",
    "    \n",
    "    return processed_song\n",
    "\n",
    "sesame_street_lyrics = process_song(sesame_street_lyrics)\n",
    "my_shoe_lyrics = process_song(my_shoe_lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89aa6ab",
   "metadata": {},
   "source": [
    "## Tokenizer and Model (for both questions and context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13fa9ea",
   "metadata": {},
   "source": [
    "Why Use BERT Instead of DPR?\n",
    "Because:\n",
    "\n",
    "- The task is more semantic and classification-oriented:\n",
    "The goal is to check whether a piece of text has certain attributes (e.g., violent content), not to retrieve the most relevant document from a large corpus.\n",
    "\n",
    "- BERT is sufficient and more general-purpose for computing mean embeddings:\n",
    "It captures general semantic information well for tasks like similarity and classification.\n",
    "\n",
    "- DPR would be overkill for this use case:\n",
    "It's optimized for retrieval scenarios involving millions of documents and queries, not for small-scale semantic matching.\n",
    "\n",
    "- BERT is more stable on small datasets:\n",
    "Since it's not fine-tuned for any specific retrieval task, it performs more consistently across varied inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a58d9c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1281e25d",
   "metadata": {},
   "source": [
    "Usage example of tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b7e94cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "[CLS] this is an example sentence for bert embeddings. [SEP] how do you like it [SEP]\n"
     ]
    }
   ],
   "source": [
    "input_text = [(\"This is an example sentence for BERT embeddings.\", \"How do you like it \"),(\"There are other models\")]\n",
    "tokens_info = bert_tokenizer(input_text,\n",
    "                            add_special_tokens = True,\n",
    "                            padding = True,\n",
    "                            truncation = True,\n",
    "                            return_tensors = 'pt')\n",
    "\n",
    "print(f\"Mask: {tokens_info['attention_mask']}\")\n",
    "text = bert_tokenizer.decode(tokens_info['input_ids'][0])\n",
    "print(text)\n",
    "\n",
    "word_embedding = bert_model(**tokens_info).pooler_output # shape torch.Size([2, 768])\n",
    "token_embedding = bert_model(**tokens_info).last_hidden_state # shape torch.Size([2, 20, 768])\n",
    "                                                              # 20 max len(tokens_info['input_ids'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fea15bc",
   "metadata": {},
   "source": [
    "The following function will not be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74c6d370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_emb(input_text, tokenizer = bert_tokenizer, model = bert_model, max_length_input = 512):\n",
    "    tokens_info = tokenizer(input_text,\n",
    "                        add_special_tokens = True,\n",
    "                        padding = True,\n",
    "                        truncation = True,\n",
    "                        max_length = max_length_input,\n",
    "                        return_tensors='pt')\n",
    "\n",
    "    input_ids = tokens_info['input_ids'].to(device)\n",
    "    attention_mask = tokens_info['attention_mask'].to(device)\n",
    "\n",
    "    word_embedding = bert_model(input_ids, attention_mask).pooler_output\n",
    "    return word_embedding  # shape: [len(list_of_text), 768]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d468f75b",
   "metadata": {},
   "source": [
    "Usage example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d46b6ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8875, -0.4939, -0.8772,  ..., -0.8416, -0.7311,  0.8905],\n",
       "        [-0.7748, -0.2365,  0.1868,  ...,  0.1140, -0.5093,  0.7607]],\n",
       "       grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = [(\"This is an example sentence for BERT embeddings.\", \"How do you like it \"),(\"There are other models\")]\n",
    "\n",
    "word_embedding = text_to_emb(input_text, bert_tokenizer, bert_model, 512)\n",
    "\n",
    "word_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80a4d01",
   "metadata": {},
   "source": [
    "## Aggregate with mean for retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b13c46c",
   "metadata": {},
   "source": [
    "Here, you'll compute aggregated mean embeddings for input sequences using the BERT model you just loaded. It processes each pair of token IDs and attention masks from the input data, extracts word embeddings for non-padded tokens, and calculates their mean. The result is a list of mean embeddings for each sequence, which is then concatenated into a single tensor. This process allows for the generation of simplified yet informative representations of the input sequences, useful for tasks like clustering, similarity search, or as input to downstream models. Each document must be under 512 tokens.\n",
    "\n",
    "Below, we define a function that does the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b83e6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_embeddings(input_text, tokenizer = bert_tokenizer, model = bert_model, max_length_input = 512):\n",
    "\n",
    "    tokens_info = tokenizer(input_text,\n",
    "                            add_special_tokens = True,\n",
    "                            padding = True,\n",
    "                            truncation = True,\n",
    "                            max_length = max_length_input,\n",
    "                            return_tensors = 'pt')\n",
    "    \n",
    "    input_ids = tokens_info['input_ids'].to(device) # shape: [len(input_text), max_seq_len]\n",
    "    attention_mask = tokens_info['attention_mask'].to(device) # shape: [len(input_text), max_seq_len]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        token_embedding = model(input_ids, attention_mask).last_hidden_state # shape: [len(input_text), max_seq_len, hidden_dim] \n",
    "\n",
    "    # Mask padding tokens\n",
    "    attention_mask_expanded = attention_mask.unsqueeze(-1)  # shape: [len(input_text), max_seq_len, 1]\n",
    "    masked_hidden = token_embedding * attention_mask_expanded # Hadamard product, shape: [len(input_text), max_seq_len, 1]\n",
    "    lengths = attention_mask_expanded.sum(dim = 1)  # shape: [len(input_text), 1]\n",
    "\n",
    "    # Calcola la media degli embedding validi\n",
    "    mean_embeddings = masked_hidden.sum(dim=1) / lengths\n",
    "    return mean_embeddings  # shape: [len(input_text), hidden_dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdc1af5",
   "metadata": {},
   "source": [
    "Usage example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e9dd3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0122, -0.3505, -0.0670,  ..., -0.0368, -0.1658,  0.3197],\n",
       "        [-0.1538, -0.1765,  0.1797,  ..., -0.1320,  0.3122,  0.0646]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = [(\"This is an example sentence for BERT embeddings.\", \"How do you like it \"),(\"There are other models\")]\n",
    "\n",
    "mean_embeddings = aggregate_embeddings(input_text, bert_tokenizer, bert_model, 512)\n",
    "\n",
    "mean_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bda2a1",
   "metadata": {},
   "source": [
    "## Embeddings and answer retrival"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c39cb2f",
   "metadata": {},
   "source": [
    "**Very important**: notice the Q and A are associated, this is the reason for which the answers embedding will not be used. In general, if the answers are not associated 1-1 to the questions, or if for each question there are multiple answers, then it should be useful to compute all the 3 products (Q-A, Q-song, A-song), and combine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53ae11cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_questions = aggregate_embeddings(song_questions) #torch.Size([8, 768])\n",
    "# embeddings_responses = aggregate_embeddings(yes_responses) #torch.Size([8, 768]) --> not used\n",
    "\n",
    "#songs\n",
    "embeddings_sesame_street = aggregate_embeddings(sesame_street_lyrics) #torch.Size([1, 768])\n",
    "embeddings_my_shoe = aggregate_embeddings(my_shoe_lyrics) #torch.Size([1, 768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16a4768e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_QA_with_similarity(embeddings_questions, embeddings_songs, n_responses = 3, distance = \"dot\"):\n",
    "    question_norms = torch.norm(embeddings_questions, dim=1, keepdim=True)\n",
    "    response_norms = torch.norm(embeddings_songs, dim=1, keepdim=True)\n",
    "\n",
    "\n",
    "    # Calculate the dot product between the question embeddings and the provided embeddings_songs (transpose of the second matrix for proper alignment).\n",
    "    dot_product = embeddings_questions @ embeddings_songs.T\n",
    "\n",
    "    \n",
    "    # Calculate cosine similarity by dividing the dot product by the product of the magnitudes\n",
    "    cosine_similarity = dot_product / (question_norms * response_norms)\n",
    "\n",
    "    # Flatten the cosine similarity tensor to a 1D tensor for easier processing\n",
    "    cosine_similarity = cosine_similarity.reshape(-1)\n",
    "    \n",
    "    # Reshape the dot product results to a 1D tensor for easier processing.\n",
    "    dot_product = dot_product.reshape(-1)\n",
    "\n",
    "    if distance == \"dot\":\n",
    "        # Sort the indices of the dot product results in descending order (setting descending to False should be True for typical similarity tasks).\n",
    "        sorted_indices = torch.argsort(dot_product, descending = True)\n",
    "\n",
    "    if distance == \"cosine\":\n",
    "        sorted_indices = torch.argsort(cosine_similarity, descending =True)\n",
    "\n",
    "    # Convert sorted indices to a list for easier iteration.\n",
    "    sorted_indices = sorted_indices.tolist()\n",
    "\n",
    "    # Print the top 'n_responses' responses from the sorted list, which correspond to the highest dot product values.\n",
    "    for index in sorted_indices[:n_responses]:\n",
    "        print(yes_responses[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36cad733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the messages conveyed in this song are positive and uplifting, promoting values like kindness, friendship, and positivity, beneficial for children.\n",
      "Yes, this song offers significant educational value, including segments that teach the alphabet, basic math, and other learning content, making it both fun and educational for children.\n",
      "Yes, this song includes explicit lyrics or bad words that might be considered offensive or inappropriate for young audiences.\n"
     ]
    }
   ],
   "source": [
    "semantic_QA_with_similarity(embeddings_questions, embeddings_sesame_street, n_responses = 3, distance = 'dot')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Trans_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
