{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2da8dfa8",
   "metadata": {
    "id": "2da8dfa8"
   },
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3665d493",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "3665d493"
   },
   "outputs": [],
   "source": [
    "########################## UTILITY AND SYSTEM ##########################\n",
    "\n",
    "import os                       # filesystem operations\n",
    "import csv                      # reading/writing CSV files\n",
    "import json                     # JSON parsing and serialization\n",
    "import math                     # basic math functions\n",
    "import random                   # random number generation\n",
    "import time                     # time-related functions\n",
    "import tempfile                 # temporary file management\n",
    "import tarfile                  # tar archive handling\n",
    "import io                       # input/output streams\n",
    "import pickle                   # object serialization\n",
    "import importlib                # dynamic import of modules\n",
    "import multiprocessing          # parallel process management\n",
    "import pkg_resources            # package and dependency management\n",
    "from copy import deepcopy       # deep copy of objects\n",
    "from pathlib import Path        # filesystem paths handling (cross-platform)\n",
    "\n",
    "########################## DOWNLOAD ##########################\n",
    "\n",
    "import requests                 # HTTP requests library\n",
    "import wget                     # file downloads from URLs\n",
    "from urllib.request import urlopen  # open URLs (alternative to requests)\n",
    "\n",
    "########################## VISUALIZATION ##########################\n",
    "\n",
    "import matplotlib.pyplot as plt # basic plotting library\n",
    "import plotly.graph_objs as go  # interactive plotting\n",
    "from tqdm.notebook import tqdm  # progress bars for loops in notebooks\n",
    "from pprint import pprint       # formatted pretty-printing of objects\n",
    "\n",
    "########################## DATAFRAME ##########################\n",
    "\n",
    "import numpy as np              # numerical arrays and operations\n",
    "import pandas as pd             # dataframes and data manipulation\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "########################## TEXT PROCESSING ##########################\n",
    "\n",
    "import re                      # regular expressions\n",
    "import string                  # string constants and operations\n",
    "from itertools import chain, islice  # advanced iteration and chaining\n",
    "\n",
    "########################## TOKENIZATION ##########################\n",
    "\n",
    "from collections import Counter, OrderedDict  # frequency counts and ordered dictionaries\n",
    "import nltk                                   # natural language processing toolkit\n",
    "from nltk.tokenize import word_tokenize       # word tokenization\n",
    "import spacy                                  # advanced NLP (tokenization, parsing)\n",
    "from torchtext.data.utils import get_tokenizer       # torchtext tokenizers\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator # build vocabulary from iterator\n",
    "\n",
    "########################## DATASET AND DATALOADER ##########################\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split   # datasets and data loading utilities\n",
    "from torch.nn.utils.rnn import pad_sequence                      # padding variable-length sequences\n",
    "from datasets import load_dataset, DatasetDict                   # HuggingFace datasets loading\n",
    "from torchtext.datasets import AG_NEWS                           # torchtext built-in datasets\n",
    "\n",
    "########################## PYTORCH AND DEEP LEARNING ##########################\n",
    "\n",
    "import torch                             # PyTorch main library\n",
    "from torch import nn, Tensor             # neural network modules and tensors\n",
    "from torch.nn import CrossEntropyLoss    # common loss function for classification\n",
    "\n",
    "########################## WORD EMBEDDING ##########################\n",
    "\n",
    "from torchtext.vocab import GloVe        # pretrained GloVe embeddings\n",
    "# from gensim.models import Word2Vec     # word2vec embeddings from corpus (commented out)\n",
    "\n",
    "########################## HUGGING FACE ##########################\n",
    "\n",
    "import transformers                      # transformers library core\n",
    "from transformers import (\n",
    "    GPT2Tokenizer, GPT2LMHeadModel,     # GPT-2 tokenizer and model\n",
    "    BertTokenizer, BertTokenizerFast, BertConfig, BertForMaskedLM,  # BERT components\n",
    "    XLNetTokenizer,                     # XLNet tokenizer\n",
    "    DistilBertForSequenceClassification, DistilBertTokenizer, AutoModelForSequenceClassification,\n",
    "    pipeline,                          # easy pipelines for inference\n",
    "    AutoTokenizer,                    # auto tokenizer loader\n",
    "    AutoModelForCausalLM, GPT2ForSequenceClassification,\n",
    "    DataCollatorForLanguageModeling, TrainingArguments, Trainer,  # training utilities\n",
    "    set_seed, GenerationConfig,\n",
    "    BertModel                        # BERT base model\n",
    ")\n",
    "from datasets import DatasetDict         # HuggingFace dataset dictionaries\n",
    "\n",
    "######################### TRL & PEFT (TRAINING & PARAMETER EFFICIENT FINE-TUNING) ##########################\n",
    "\n",
    "# from trl import (\n",
    "#     SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM,\n",
    "#     DPOConfig, DPOTrainer,\n",
    "#     RewardTrainer, RewardConfig\n",
    "# )\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from torchmetrics import Accuracy        # metrics for evaluation\n",
    "\n",
    "########################## RAG ##########################\n",
    "\n",
    "from transformers import (\n",
    "    DPRQuestionEncoder, DPRQuestionEncoderTokenizer,\n",
    "    DPRContextEncoder, DPRContextEncoderTokenizer\n",
    ")\n",
    "import faiss                              # similarity search library\n",
    "\n",
    "########################## EVALUATION ##########################\n",
    "\n",
    "import evaluate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "bca7a467",
   "metadata": {
    "executionInfo": {
     "elapsed": 357760,
     "status": "aborted",
     "timestamp": 1752786900757,
     "user": {
      "displayName": "Alessio Fontanarossa",
      "userId": "15715623213071598062"
     },
     "user_tz": -120
    },
    "id": "bca7a467"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which device we are on: cpu\n"
     ]
    }
   ],
   "source": [
    "def accelerator(where = \"mps\"):\n",
    "    if where == \"mps\":\n",
    "        device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        print(\"Which device we are on: {}\".format(device))\n",
    "        return device\n",
    "    if where == \"cuda\":\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"Which device we are on: {}\".format(device))\n",
    "        return device\n",
    "    if where == \"cpu\":\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Which device we are on: {}\".format(device))\n",
    "        return device\n",
    "\n",
    "device = accelerator(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42cc614",
   "metadata": {
    "id": "e42cc614"
   },
   "source": [
    "# 1) CONCEPTS: Positional encoding, Attention, Self-Attention, Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f63fcf",
   "metadata": {
    "id": "e0f63fcf"
   },
   "source": [
    "## Positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a297414",
   "metadata": {
    "id": "2a297414"
   },
   "source": [
    "It is needed to distinguish 'He painted the car red', which is different from 'He painted the red car', but the vector representation is the same. Thus, we need\n",
    "- positional encoding: given a sentence with $N$ tokens ($pos=0 ,\\ldots,N-1$), $h$ in the hidden dimension ($i=0,\\ldots,h/2 -1$), we add $$ PE(pos,2i)=\\sin\\Big(\\frac{pos}{10000^{2i/h}}\\Big)\\,,\\quad  PE(pos,2i+1)=\\cos\\Big(\\frac{pos}{10000^{2i/h}}\\Big),$$ to the embedding value of the token, position per position. Occasionally, for practicality, it is also useful to add padding to arrive to the vocabulary lenght. **We use the sine and cosine because they are limited functions, and they do not explode for very very long sequences** Simple example. in which then you have to perform the sum:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>\n",
    "\n",
    "### Example: h = 4 and N = 3\n",
    "\n",
    "| Word        | Col1 | Col2 | Col3 | Col4 |\n",
    "|-------------|------|------|------|------|\n",
    "| transformers| 0.2  | 0.4  | 0.1  | 0.3  |\n",
    "| are         | 0.5  | 0.2  | 0.7  | 0.9  |\n",
    "| awesome     | 0.8  | 0.6  | 0.4  | 0.2  |\n",
    "\n",
    "</td>\n",
    "    <td>\n",
    "\n",
    "### Positional encoding\n",
    "\n",
    "| Word        | PE0  | PE1 | PE2  | PE3  |\n",
    "|-------------|------|------|------|------|\n",
    "| transformers| $\\sin\\Big(\\frac{0}{10000^{2i/h}}\\Big)=$ 0  | $\\cos\\Big(\\frac{0}{10000^{2i/h}}\\Big)=$ 1  | $\\sin\\Big(\\frac{0}{10000^{2i/h}}\\Big)=$ 0  | $\\cos\\Big(\\frac{0}{10000^{2i/h}}\\Big)=$ 1  |\n",
    "| are         | $\\sin\\Big(\\frac{1}{10000^{2i/h}}\\Big)=$ 0.84 | $\\cos\\Big(\\frac{1}{10000^{2i/h}}\\Big)=$ 0.54 | $\\sin\\Big(\\frac{1}{10000^{2i/h}}\\Big)=$ 0.01 | $\\cos\\Big(\\frac{1}{10000^{2i/h}}\\Big)=$ 0.99  |\n",
    "| awesome     | $\\sin\\Big(\\frac{2}{10000^{2i/h}}\\Big)=$ 0.90 | $\\cos\\Big(\\frac{2}{10000^{2i/h}}\\Big)=$ -0.41 | $\\sin\\Big(\\frac{2}{10000^{2i/h}}\\Big)=$ 0.02 | $\\cos\\Big(\\frac{2}{10000^{2i/h}}\\Big)=$ 0.99  |\n",
    "\n",
    "</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "- Dinamical positional encoding: the positional encoding are learnable parameter (GPT)\n",
    "- Segment encoding: used in BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd3365c",
   "metadata": {
    "id": "ebd3365c"
   },
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1823ad",
   "metadata": {
    "id": "4d1823ad"
   },
   "source": [
    "We can use OHE vectors or embedded vectors. Here **K=keys**,**Q=queries** and **V=values** and we have to think to a pPython dictionary, which is dict={K:V} and we acceed to it by doing a query dict[q] (q is one among the K):\n",
    "\n",
    "**OHE**:\n",
    "1. $k_{le} = (0,0,0,1,0,0)^t$, $k_{chat}=(1,0,0,0,0,0)^t$, $k_{est}=(0,1,0,0,0,0)^t$, $k_{sous}=(0,0,0,0,1,0)^t$, $k_{la}=(0,0,1,0,0,0)^t$, $k_{table}=(0,0,0,0,0,1)^t$ and we form the matrix **$K_{n \\times d_k}$** (each row is a $k_i$), where **$d_k$** is the dimension of the keys vocabulary and **$n$** is the number of token;\n",
    "2. same for the queries vectors, and we build **$Q_{n \\times d_k}=K_{n \\times d_k}$**;\n",
    "3. Similar for the values, and the matrix will be **$V_{n \\times d_v}$**, where **$d_v$** is the dimension of the keys vocabulary\n",
    "\n",
    "Then for a specific query $q_i$, we have\n",
    "\n",
    "$$\\text{Attention}(q_i, K,V) \\equiv (h_i^t)_{1 \\times d_v}= (q_i^t)_{1 \\times d_k} \\cdot (K^t)_{d_k \\times n} \\cdot (V)_{n \\times d_v}\\,.$$\n",
    "\n",
    "Due to orthogonality, the vector $h_i$ is exactly equal to $v_i$, so this attention is a way to perform a query on a dictionary rapidly. To retrieve the word from the OHE we do\n",
    "\n",
    "$$ \\text{word} = \\argmax_{\\text{index}}(h_i^t )\\,.$$\n",
    "\n",
    "**Word Embedding**:\n",
    "\n",
    "Everything is very similar, but **$K_{n \\times h_k}$** and **$V_{n \\times h_v}$**, where **h_{k,v}** are the hidden dimensions of keys and values. Now we use a softmax to mimic the behaviour of OHE vectors:\n",
    "\n",
    "$$\\text{Attention}(q_i, K,V) \\equiv (h_i^t)_{1 \\times d_v}= \\text{softmax}\\Big[(q_i^t)_{1 \\times d_k} \\cdot (K^t)_{d_k \\times n}\\Big] \\cdot (V)_{n \\times d_v}\\,.$$\n",
    "\n",
    "The piece in $\\Big[\\quad\\Big]$ is like $(0.01, 0.02,---,0.99,---,0.001)$. Similarly\n",
    "\n",
    "$$ \\text{word} = \\argmax_{\\text{index}}(h_i^t )\\,.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d2b415",
   "metadata": {
    "id": "f4d2b415"
   },
   "source": [
    "If instead of word embedding we want to embed sequences, everything remains the same but we replace $q_i$ with an aggregate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660a01fa",
   "metadata": {
    "id": "660a01fa"
   },
   "source": [
    "## Self-Attention, multi-head, Trasformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6bd799",
   "metadata": {
    "id": "0a6bd799"
   },
   "source": [
    "It is like the Attention mechanism, but we add a (final) linear layer:\n",
    "\n",
    "$$ \\text{Tokens} \\rightarrow \\text{Embedding linear layer} \\rightarrow \\text{embedded version of token} \\rightarrow \\text{Linear layer with weights W} \\rightarrow \\text{New space for Queries, Keys and values}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c9977f",
   "metadata": {
    "id": "22c9977f"
   },
   "source": [
    "This is particularly useful in the transformer (ecnoder + decoder):\n",
    "\n",
    "$$\\text{Attention}(q_i, K,V) \\equiv (h_i^t)_{1 \\times d_v}= \\text{softmax}\\bigg[\\frac{(q_i^t)_{1 \\times d_k} \\cdot (K^t)_{d_k \\times n}}{\\sqrt{d_k}}\\bigg] \\cdot (V)_{n \\times d_v}\\,.$$\n",
    "\n",
    "### ðŸ”„ Encoder-Decoder Attention: Q, K, V Overview\n",
    "\n",
    "| Role   | Source   | Vocabulary       | Embedding                                  | Projection Matrix                                   | Resulting Shape             |\n",
    "|--------|----------|------------------|---------------------------------------------|-----------------------------------------------------|-----------------------------|\n",
    "| **K**  | Encoder  | Source language  | $ E_{\\text{src}} \\in \\mathbb{R}^{n \\times d_{\\text{model}}} $ | $ W^K \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k} $ | $ K=E_{\\text{src}}\\cdot W^K  \\in \\mathbb{R}^{n \\times d_k} $ |\n",
    "| **V**  | Encoder  | Source language  | $ E_{\\text{src}} \\in \\mathbb{R}^{n \\times d_{\\text{model}}} $ | $ W^V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_v} $ | $ V=E_{\\text{src}}\\cdot W^V  \\in \\mathbb{R}^{n \\times d_v} $ |\n",
    "| **Q**  | Decoder  | Target language  | $ E_{\\text{tgt}} \\in \\mathbb{R}^{m \\times d_{\\text{model}}} $ | $ W^Q \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k} $ | $ Q=E_{\\text{tgt}}\\cdot W^Q \\in \\mathbb{R}^{m \\times d_k} $ |\n",
    "\n",
    "### ðŸ§¾ Legend\n",
    "\n",
    "- $ n $: number of tokens in the **source** sequence (encoder input)\n",
    "- $ m $: number of tokens in the **target** sequence generated so far (decoder input)\n",
    "- $ d_{\\text{model}} $: embedding dimension used throughout the model\n",
    "- $ d_k $: dimension of the **query/key** space\n",
    "- $ d_v $: dimension of the **value** space\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f780fc3d",
   "metadata": {
    "id": "f780fc3d"
   },
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2facc8",
   "metadata": {
    "id": "6e2facc8"
   },
   "source": [
    "Now we can use a number of heads $n_h$, which are a $n_h$ different layers with weights $W_{i=1,\\ldots,n_h}$ for $\\{K,V,Q\\}$. The output of the layers will be concatenated and then goes to a final ouput layer $W^O$. So, for example,\n",
    "\n",
    "$$ \\text{MultiHead}(Q,K,V)=Concat(\\text{head}_{1}â€‹,\\ldots,\\text{head}_{n_h}â€‹)W^O\\,,\\quad \\text{head}_{i}= \\text{softmax}\\bigg[\\frac{(Q_i^t)_{m \\times d_k} \\cdot (K_i^t)_{d_k \\times n}}{\\sqrt{d_k}}\\bigg] \\cdot (V_i)_{n \\times d_v}\\,.$$\n",
    "\n",
    "with, for example,\n",
    "\n",
    "$$V_i = E_{\\text{src}}\\cdot W_i^V\\,,\\quad i=1,\\ldots,n_h\\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27b3d93",
   "metadata": {
    "id": "e27b3d93"
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029e12f7",
   "metadata": {
    "id": "029e12f7"
   },
   "source": [
    "The following is a typical representation of the architecture of encoder - decoder. In particular, this is a 'single cell' or 'single temporal step', so in a real encoder there is a number of cell like this one. The 'outputs' under the decoder is the generated token from the decoder at the previous step! Finally, this gray cell is a type of RNN. Add & norm is useful to reduce problems with gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82981c5c",
   "metadata": {
    "id": "82981c5c"
   },
   "source": [
    "![Logo OpenAI](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*376uJu_fc_uR8H3X.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff590657",
   "metadata": {
    "id": "ff590657"
   },
   "source": [
    "# 2) Useful Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0006e77b",
   "metadata": {
    "id": "0006e77b"
   },
   "source": [
    "![Logo OpenAI](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*376uJu_fc_uR8H3X.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0317c3f2",
   "metadata": {
    "id": "0317c3f2"
   },
   "source": [
    "To understand the above picture (where recall that the cell is a single RNN and not the full encoder) and the PyTorch classes used, we go through the following steps:\n",
    "\n",
    "1. **Self-attention**, which is at the hearth of the encoder-decoder. This can be done defining an Head class manually, but more properly there is the `nn.MultiheadAttention` method from PyTorch;\n",
    "2. **Positional Encoding**\n",
    "3. Finally a full encoder-decoder! For this we have many classes\n",
    "\n",
    "| Class                          | Main Purpose                                                             | Internal Components                                                                 | When to Use                                                              |\n",
    "|-------------------------------|---------------------------------------------------------------------------|--------------------------------------------------------------------------------------|---------------------------------------------------------------------------|\n",
    "| `nn.Transformer`              | Full Transformer model (Encoder + Decoder).                              | Includes `TransformerEncoder` + `TransformerDecoder`.                               | For sequence-to-sequence tasks like translation or summarization.        |\n",
    "| `nn.TransformerEncoderLayer` | A **single** encoder layer: self-attention + feed-forward + normalization.| `MultiheadAttention` (self-attn), dropout, LayerNorm, feed-forward (FFN), residuals. | To manually build or inspect an encoder block.                           |\n",
    "| `nn.TransformerEncoder`      | A **stack** of encoder layers.                                           | Repeats `TransformerEncoderLayer` N times (can share or not share weights).         | For pure encoder tasks like text classification, embeddings (e.g. BERT). |\n",
    "| `nn.TransformerDecoderLayer` | A **single** decoder layer: self-attn + encoder-decoder attn + FFN.       | Self-attn, cross-attn (to encoder), dropout, LayerNorm, FFN, residuals.              | To inspect or customize a decoder block manually.                        |\n",
    "| `nn.TransformerDecoder`      | A **stack** of decoder layers.                                           | Repeats `TransformerDecoderLayer` N times.                                          | Use when building the decoder part of a full Transformer model.          |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ec0e89",
   "metadata": {
    "id": "56ec0e89"
   },
   "source": [
    "In particular, notice that for `nn.TransformerEncoderLayer` is a single RNN, so the gray cell, and also that it contains already the multiheadAttention mechanism. It can be accessed using\n",
    "```\n",
    "layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "print(layer.self_attn)\n",
    "```\n",
    "\n",
    "A full decoder is a stack of N-RNN, which is nn.transformerEncoder!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9652ef",
   "metadata": {
    "id": "4c9652ef"
   },
   "source": [
    "Let's see them (the classes) one-by-one:\n",
    "\n",
    "```\n",
    "multihead_attention = nn.MultiheadAttention(embed_dim, num_heads,batch_first = False)\n",
    "query = torch.rand((seq_length, batch_size, embed_dim))\n",
    "key = torch.rand((seq_length, batch_size, embed_dim))\n",
    "value = torch.rand((seq_length, batch_size, embed_dim))\n",
    "attention_output, _ = multihead_attn(query, key, value)\n",
    "```\n",
    "\n",
    "```\n",
    "transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\n",
    "src = torch.rand((seq_length_in, batch_size, embed_dim))\n",
    "tgt = torch.rand((seq_length_out, batch_size, embed_dim))\n",
    "out = transformer_model(src, tgt)\n",
    "```\n",
    "\n",
    "```\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model = embed_dim, nhead = num_heads) # one encoder\n",
    "\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers = num_layers) # stack of num_layers encoders\n",
    "src = torch.rand((seq_length_in, batch_size, embed_dim))\n",
    "encoded = transformer_encoder(src)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86394bf7",
   "metadata": {
    "id": "86394bf7"
   },
   "source": [
    "Consider an **encoder** in the figure. The inputs are in the form <span style=\"background-color: yellow\">([tl = tokens_lenghts, bs = number of samples in a batch = batch_size])</span> , ans recall that all the sequences in a batch must have the same number of tokens (eventually, use padding). Then we use an `nn.Embedding` layer, so after the pink we have <span style=\"background-color: yellow\">([tl, bs, ed = embeding_dimension])</span>. The positional encoding does not change the dimensions, and also the ancoder. So the output of the encoder, which is called **contextual embedding** is <span style=\"background-color: yellow\">([tl, bs, ed])</span>. Then usually one takes ` mean(dim = 0)` to arrive to <span style=\"background-color: yellow\">([bs, ed]) </span>, and finally the linear layer (for example a classifier) arrives to the number of classes <span style=\"background-color: yellow\">([bs, num_classes]) </span> in the classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60189b25",
   "metadata": {
    "id": "60189b25"
   },
   "source": [
    "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX05RNEN/Tokenization%20-%20Color.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfdc814",
   "metadata": {
    "id": "1cfdc814"
   },
   "source": [
    "Masked self-attention means that the model only attends to the previous tokens in the sequence for predicting the next token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9b05b3",
   "metadata": {
    "id": "6f9b05b3"
   },
   "source": [
    "# 3) (Self-)attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c6c8ed",
   "metadata": {
    "id": "e0c6c8ed"
   },
   "source": [
    "## Using OHE (softmax not necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "06e14860",
   "metadata": {
    "id": "06e14860",
    "outputId": "f30a1815-a8d3-43a4-a800-16c16cb359bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': tensor([1., 0., 0., 0., 0.]),\n",
       " 'is': tensor([0., 1., 0., 0., 0.]),\n",
       " 'table': tensor([0., 0., 1., 0., 0.]),\n",
       " 'the': tensor([0., 0., 0., 1., 0.]),\n",
       " 'under': tensor([0., 0., 0., 0., 1.])}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = {\n",
    "    'le': 'the'\n",
    "    , 'chat': 'cat'\n",
    "    , 'est': 'is'\n",
    "    , 'sous': 'under'\n",
    "    , 'la': 'the'\n",
    "    , 'table': 'table'\n",
    "}\n",
    "vocabulary_in = sorted(list(set(dictionary.keys())))\n",
    "vocabulary_out = sorted(list(set(dictionary.values()))) #set uses the unique values\n",
    "\n",
    "def encode_one_hot(vocabulary):\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    one_hot = dict()\n",
    "    LEN = len(vocabulary)\n",
    "\n",
    "    for i, key in enumerate(vocabulary):\n",
    "        one_hot_vector = torch.zeros(LEN)  # Start with a vector of zeros\n",
    "        one_hot_vector[i] = 1  # Set the i-th position to 1 for the current word\n",
    "        one_hot[key] = one_hot_vector  # Map the word to its one-hot encoded vector\n",
    "\n",
    "    return one_hot  # Return the dictionary of words and their one-hot encoded vectors\n",
    "\n",
    "one_hot_in = encode_one_hot(vocabulary_in)\n",
    "one_hot_out = encode_one_hot(vocabulary_out)\n",
    "\n",
    "one_hot_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96882e2d",
   "metadata": {
    "id": "96882e2d"
   },
   "source": [
    "We can now create the fixed matrices K,V to mimic the use of a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "cdb4718c",
   "metadata": {
    "id": "cdb4718c",
    "outputId": "3e9b6b5b-acbe-43d7-9b09-31b5d446933c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 1.])\n",
      "tensor([0.1296, 0.1296, 0.1296, 0.2591, 0.3522])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/35/gw8dmgsd6m11bg8nhrgpd3vr0000gn/T/ipykernel_69575/3583722038.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention_refined = nn.functional.softmax(q @ K.T) @ V\n"
     ]
    }
   ],
   "source": [
    "K = torch.stack([one_hot_in[k] for k in dictionary.keys()])\n",
    "V = torch.stack([one_hot_out[k] for k in dictionary.values()])\n",
    "\n",
    "q = one_hot_in['sous']\n",
    "\n",
    "attention = (q @ K.T) @ V\n",
    "\n",
    "print(attention)\n",
    "\n",
    "attention_refined = nn.functional.softmax(q @ K.T) @ V\n",
    "\n",
    "print(attention_refined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25b8c72",
   "metadata": {
    "id": "b25b8c72"
   },
   "source": [
    "Finally we extract the predicted word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "316a2d4c",
   "metadata": {
    "id": "316a2d4c",
    "outputId": "5ddd2814-f92f-41c5-bf98-fac36e4ea249"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4) tensor(4)\n",
      "under under\n"
     ]
    }
   ],
   "source": [
    "prediction_number = torch.argmax(attention)\n",
    "prediction_number_refined = torch.argmax(attention)\n",
    "\n",
    "print(prediction_number, prediction_number_refined)\n",
    "\n",
    "predicted_word = vocabulary_out[prediction_number]\n",
    "predicted_word_refined = vocabulary_out[prediction_number_refined]\n",
    "\n",
    "print(predicted_word, predicted_word_refined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af413e21",
   "metadata": {
    "id": "af413e21"
   },
   "source": [
    "## Creating a single-head class from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1007cc2",
   "metadata": {
    "id": "e1007cc2"
   },
   "source": [
    "| Role   | Source   | Vocabulary       | Embedding                                  | Projection Matrix                                   | Resulting Shape             |\n",
    "|--------|----------|------------------|---------------------------------------------|-----------------------------------------------------|-----------------------------|\n",
    "| **K**  | Encoder  | Source language  | $ E_{\\text{src}} \\in \\mathbb{R}^{n \\times d_{\\text{model}}} $ | $ W^K \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k} $ | $ K=E_{\\text{src}}\\cdot W^K  \\in \\mathbb{R}^{n \\times d_k} $ |\n",
    "| **V**  | Encoder  | Source language  | $ E_{\\text{src}} \\in \\mathbb{R}^{n \\times d_{\\text{model}}} $ | $ W^V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_v} $ | $ V=E_{\\text{src}}\\cdot W^V  \\in \\mathbb{R}^{n \\times d_v} $ |\n",
    "| **Q**  | Decoder  | Target language  | $ E_{\\text{tgt}} \\in \\mathbb{R}^{m \\times d_{\\text{model}}} $ | $ W^Q \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k} $ | $ Q=E_{\\text{tgt}}\\cdot W^Q \\in \\mathbb{R}^{m \\times d_k} $ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b615136",
   "metadata": {
    "id": "4b615136"
   },
   "source": [
    "self attention with embedding layer + linear layer (learnable parameter). This is a simple model where\n",
    "\n",
    "$$ d_{\\text{model}} \\rightarrow n_{\\text{embd}}\\,,\\quad d_k=d_v \\rightarrow n_{\\text{embd}}\\,, \\quad n,m \\rightarrow \\text{vocab\\_size}\\,.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0b2c172d",
   "metadata": {
    "id": "0b2c172d"
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, n_embd):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, n_embd)\n",
    "\n",
    "        self.key = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        self.query = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        self.value = nn.Linear(n_embd, n_embd, bias=False)\n",
    "\n",
    "    def attention(self, x):\n",
    "        embedded_x = self.embedding(x)\n",
    "\n",
    "        k = self.key(embedded_x)\n",
    "        q = self.query(embedded_x)\n",
    "        v = self.value(embedded_x)\n",
    "        w = nn.functional.softmax(q @ k.transpose(-2, -1) * k.shape[-1]**-0.5 , dim=-1) #k.shape[-1] = n_embd in this case!\n",
    "        return embedded_x, k, q, v, w\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded_x = self.embedding(x)\n",
    "\n",
    "        k = self.key(embedded_x)\n",
    "        q = self.query(embedded_x)\n",
    "        v = self.value(embedded_x)\n",
    "\n",
    "        attention = nn.functional.softmax(q @ k.transpose(-2, -1) * k.shape[-1]**-0.5 , dim=-1) @ v # Do a softmax across the last dimesion\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79795f10",
   "metadata": {
    "id": "79795f10"
   },
   "source": [
    "Dataset and standard things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3c2a46b7",
   "metadata": {
    "id": "3c2a46b7"
   },
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    (1,\"Introduction to NLP\"), (2,\"Basics of PyTorch\"), (1,\"NLP Techniques for Text Classification\"),\n",
    "    (3,\"Named Entity Recognition with PyTorch\"), (3,\"Sentiment Analysis using PyTorch\"), (3,\"Machine Translation with PyTorch\"),\n",
    "    (1,\" NLP Named Entity,Sentiment Analysis,Machine Translation \"),(1,\" Machine Translation with NLP \"), (1,\" Named Entity vs Sentiment Analysis  NLP \"),\n",
    "    (3,\"he painted the car red\"), (1,\"he painted the red car\")]\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(dataset), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "def text_pipeline(x):\n",
    "    return vocab(tokenizer(x))\n",
    "\n",
    "tokens = [text_pipeline(text) for _, text in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c2981e74",
   "metadata": {
    "id": "c2981e74",
    "outputId": "0162756f-5f95-450d-c6be-45decbf5e627"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0112, -0.0472, -0.0447],\n",
       "        [-0.0106, -0.0316, -0.0211],\n",
       "        [-0.0022, -0.0722, -0.0378],\n",
       "        [ 0.0048, -0.0526, -0.0399],\n",
       "        [ 0.0180, -0.0383, -0.0490],\n",
       "        [-0.0086, -0.0899, -0.0358],\n",
       "        [ 0.0048, -0.0526, -0.0399],\n",
       "        [ 0.0030, -0.0633, -0.0406],\n",
       "        [ 0.0167, -0.0730, -0.0555]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = len(vocab)\n",
    "N_EMBD = 3\n",
    "\n",
    "\n",
    "attention_head = Head(vocab_size = VOCAB_SIZE, n_embd = N_EMBD)\n",
    "\n",
    "tensor_tokens = torch.tensor(tokens[6], dtype = torch.long)\n",
    "\n",
    "attention_head(tensor_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e70f105",
   "metadata": {
    "id": "6e70f105"
   },
   "source": [
    "## Using the PyTorch multi-head class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85ebcec",
   "metadata": {
    "id": "a85ebcec"
   },
   "source": [
    "`nn.MultiheadAttention` is a module in PyTorch that implements the multi-head self-attention mechanism, a key component of the Transformer architecture. This attention mechanism enables the model to focus on different parts of the input sequence simultaneously, capturing various contextual dependencies and improving the model's ability to process complex natural language patterns.\n",
    "\n",
    "The `nn.MultiheadAttention` module has three main inputs: `query`, `key`, and `value` as illustrated below.\n",
    "<p style=\"text-align:center\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/MultiHeadAttention.png\" width=\"300\" alt=\"MultiHead\"/>\n",
    "</p>\n",
    "\n",
    "The multi-head attention mechanism works by first splitting the `query`, `key`, and `value` inputs into multiple \"heads,\" each with its own set of learnable weights. This process allows the model to learn different attention patterns in parallel.\n",
    "\n",
    "The outputs from all heads are concatenated and passed through a linear layer, known as the output projection, to combine the information learned by each head. This final output represents the contextually enriched sequence that can be used in subsequent layers of the Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "508885eb",
   "metadata": {
    "id": "508885eb",
    "outputId": "0e4aca8d-60df-47b6-9308-28331a213e73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "should be zero, and it is 0\n",
      "Attention Output Shape: torch.Size([10, 5, 4])\n"
     ]
    }
   ],
   "source": [
    "embed_dim =4\n",
    "num_heads = 2\n",
    "print(\"should be zero, and it is\",embed_dim %num_heads)\n",
    "multihead_attn = nn.MultiheadAttention(embed_dim = embed_dim, num_heads = num_heads, batch_first = False)\n",
    "\n",
    "seq_length = 10 # Sequence length\n",
    "batch_size = 5 # Batch size\n",
    "query = torch.rand((seq_length, batch_size, embed_dim))\n",
    "key = torch.rand((seq_length, batch_size, embed_dim))\n",
    "value = torch.rand((seq_length, batch_size, embed_dim))\n",
    "# Perform multi-head attention\n",
    "attn_output, _= multihead_attn(query, key, value)\n",
    "print(\"Attention Output Shape:\", attn_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a734c13d",
   "metadata": {
    "id": "a734c13d"
   },
   "source": [
    "The architecture is different even if I set num_heads = 2, for example beacuse there is the concat and the final output layer. For this reason, I can not reproduce the results from the Head class above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d18b863",
   "metadata": {
    "id": "3d18b863"
   },
   "source": [
    "# 4) Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9c43a6",
   "metadata": {
    "id": "df9c43a6"
   },
   "source": [
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a24fc3",
   "metadata": {
    "id": "57a24fc3"
   },
   "source": [
    "If use `nn.TransformerEncoderLayer` in version batch-first (`batch_first = True`), use this: ([batch_size, seq_len, emb_size])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3003e44e",
   "metadata": {
    "id": "3003e44e"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # Shape: [1, max_len, d_model]\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1cb170",
   "metadata": {
    "id": "da1cb170"
   },
   "source": [
    "If use  `nn.Transformer`, use this: ([seq_len, batch_size, emb_size])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d55e394d",
   "metadata": {
    "id": "d55e394d"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bb1fe7",
   "metadata": {
    "id": "f6bb1fe7"
   },
   "source": [
    "# 5) Encoders for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb23234",
   "metadata": {
    "id": "0fb23234"
   },
   "source": [
    "In the previous notebook, we used an embedding layer (doing embeddingbag) and then a linear layer to go to 4 neurons, the number of classes of the AG_NEWS dataset\n",
    "```\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "\n",
    "        self.embedding = nn.EmbeddingBag(input_dim, hidden_dim, sparse=False)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.init_weights()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d968b4",
   "metadata": {
    "id": "b3d968b4"
   },
   "source": [
    "This time we will use an encoder, which should enhance the prestation using the attention mechanism!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404907a8",
   "metadata": {
    "id": "404907a8"
   },
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9397a9aa",
   "metadata": {
    "id": "9397a9aa"
   },
   "source": [
    "Taken exactly from notebook 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d3cf3902",
   "metadata": {
    "id": "d3cf3902"
   },
   "outputs": [],
   "source": [
    "from torchtext.datasets import AG_NEWS\n",
    "\n",
    "def text_pipeline(x):\n",
    "  return vocab(tokenizer(x))\n",
    "\n",
    "def label_pipeline(x):\n",
    "   return int(x) - 1\n",
    "\n",
    "train_iter, test_iter = iter(AG_NEWS(split=\"train\")), iter(AG_NEWS(split=\"test\"))\n",
    "ag_news_label = {1: \"World\", 2: \"Sports\", 3: \"Business\", 4: \"Sci/Tec\"}\n",
    "num_class = len(set([label for (label, text) in train_iter ]))\n",
    "\n",
    "train_iter, test_iter = iter(AG_NEWS(split=\"train\")), iter(AG_NEWS(split=\"test\"))\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train, split_valid = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "len(split_train)+len(split_valid)==len(train_dataset)\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for label, text in data_iter:\n",
    "        yield tokenizer(text.lower())  # Lowercase conversion for consistency\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dae0671",
   "metadata": {
    "id": "4dae0671"
   },
   "source": [
    "The collate function is a bit different. We will use nn.Embedding instead of nn.EmbeddingBag, so we will not be interested in the offsets but we have to manually manage the padding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b884eb33",
   "metadata": {
    "id": "b884eb33"
   },
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        text_list.append(torch.tensor(text_pipeline(_text), dtype=torch.int64))\n",
    "\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = pad_sequence(text_list, batch_first = True) #here the padding is done automatically\n",
    "\n",
    "    return label_list.to(device), text_list.to(device)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train, batch_size = BATCH_SIZE, shuffle = True, collate_fn = collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid, batch_size = BATCH_SIZE, shuffle = True, collate_fn = collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size = BATCH_SIZE, shuffle = True, collate_fn = collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ad3080",
   "metadata": {
    "id": "e3ad3080"
   },
   "source": [
    "## Accuracy and predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba85dd3f",
   "metadata": {
    "id": "ba85dd3f"
   },
   "source": [
    "Same predict function as in 2) and similar accuracy as in 2) (the only differences in these is dimensions and the fact that offsets are stripped of):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6a49df8f",
   "metadata": {
    "id": "6a49df8f"
   },
   "outputs": [],
   "source": [
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.unsqueeze(torch.tensor(text_pipeline(text)),0).to(device)\n",
    "        output = model(text)\n",
    "        return ag_news_label[output.argmax(1).item() + 1]\n",
    "\n",
    "def evaluate(dataloader, model ):\n",
    "    model.eval()\n",
    "    total_acc, total_count= 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text) in enumerate(dataloader):\n",
    "            predicted_label = model(text.to(device))\n",
    "\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba226932",
   "metadata": {
    "id": "ba226932"
   },
   "source": [
    "## Positional encoding (copied from above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ae94f008",
   "metadata": {
    "id": "ae94f008"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # Shape: [1, max_len, d_model]\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20210572",
   "metadata": {
    "id": "20210572"
   },
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0115bc73",
   "metadata": {
    "id": "0115bc73"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        num_class,\n",
    "        embedding_dim=100,\n",
    "        nhead=5,\n",
    "        dim_feedforward=2048,\n",
    "        num_layers=6,\n",
    "        dropout=0.1,\n",
    "        max_len=5000,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(\n",
    "            d_model=embedding_dim,\n",
    "            dropout=dropout,\n",
    "            max_len=max_len,\n",
    "        )\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True  # âœ… questo va messo per usare [B, T, E]\n",
    "        )\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(embedding_dim, num_class)\n",
    "        self.d_model = embedding_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len]\n",
    "        x = self.emb(x) * math.sqrt(self.d_model)  # Shape: [B, T, E]\n",
    "        x = self.pos_encoder(x)                   # Shape: [B, T, E]\n",
    "        x = self.transformer_encoder(x)           # Shape: [B, T, E]\n",
    "        x = x.mean(dim=1)                         # Average over tokens (T)\n",
    "        x = self.classifier(x)                    # Final logits [B, C]\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "467a9e11",
   "metadata": {
    "id": "467a9e11"
   },
   "outputs": [],
   "source": [
    "emsize = 64\n",
    "vocab_size=len(vocab)\n",
    "model = Net(vocab_size=vocab_size,num_class=4).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700b1d00",
   "metadata": {
    "id": "700b1d00"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76eb734",
   "metadata": {
    "id": "a76eb734"
   },
   "source": [
    "Same training (again without offsets):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a97d829",
   "metadata": {},
   "source": [
    "It works, but it is commented for time saving:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "51012eef",
   "metadata": {
    "colab": {},
    "id": "51012eef",
    "outputId": "60131a6d-9a70-4cd4-a128-6c125f513b1d"
   },
   "outputs": [],
   "source": [
    "lr=0.1\n",
    "epochs = 1\n",
    "cum_loss_list=[]\n",
    "acc_epoch=[]\n",
    "acc_old=0\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 1.0, gamma = 0.1) # after 1.0 epochs, the lerning rate will be multiplied by gamma\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     cum_loss = 0.0\n",
    "#     model.train()\n",
    "#     for i, (label, text) in enumerate(tqdm(train_dataloader)):\n",
    "#         optimizer.zero_grad()\n",
    "#         predicted_label = model(text)\n",
    "#         loss = criterion(predicted_label, label)\n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1) #gradient clipping\n",
    "#         optimizer.step()\n",
    "#         cum_loss += loss.item()\n",
    "\n",
    "#     cum_loss_list.append(cum_loss/len(train_dataloader))\n",
    "#     accu_val = evaluate(valid_dataloader, model)\n",
    "#     acc_epoch.append(accu_val)\n",
    "\n",
    "#     if accu_val > acc_old:\n",
    "#       acc_old= accu_val\n",
    "#       torch.save(model.state_dict(), 'my_model.pth')\n",
    "\n",
    "#     #scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4f7819ec",
   "metadata": {
    "id": "4f7819ec"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot(COST,ACC):\n",
    "    fig, ax1 = plt.subplots()\n",
    "    color = 'tab:red'\n",
    "    ax1.plot(COST, color=color)\n",
    "    ax1.set_xlabel('epoch', color=color)\n",
    "    ax1.set_ylabel('loss', color=color)\n",
    "    ax1.tick_params(axis='y', color=color)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('accuracy', color=color)  # you already handled the x-label with ax1\n",
    "    ax2.plot(ACC, color=color)\n",
    "    ax2.tick_params(axis='y', color=color)\n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot(cum_loss_list, acc_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca8f394",
   "metadata": {
    "id": "7ca8f394"
   },
   "source": [
    "# 6) CONCEPTS: Decoder, Encoder, Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e122e3e",
   "metadata": {
    "id": "3e122e3e"
   },
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad232bc",
   "metadata": {
    "id": "6ad232bc"
   },
   "source": [
    "Important difference between training and predicting using decoders: **Teacher Forcing**:\n",
    "\n",
    "1. During **prediction/inference** the flow is $$ \\text{token}_0 = x_0 \\rightarrow \\text{input embedding} \\rightarrow \\text{decoder} \\rightarrow \\text{contextual representation } \\hat{x}_0 \\rightarrow \\text{output layer with output } z_1 \\rightarrow \\widehat{\\text{word}}_1= \\argmax_i(z_1)\\,,$$. Then $$ (\\hat{x}_1, \\widehat{\\text{word}}_1 ) \\rightarrow \\text{input embedding}\\,.$$ Recall alo that in general, to the input layer, are sent $(\\hat{x}_t,\\widehat{\\text{word}}_t, \\widehat{\\text{word}}_{t-1},\\ldots,\\widehat{\\text{word}}_1 )$.\n",
    "\n",
    "2. During **training** instead, to the input embedding are sent all the true tokens $(x_1,\\ldots)$ at the same time, and the loss is computed using $\\mathcal{L}(\\text{word}_t,\\widehat{\\text{word}}_t)$. Here $\\text{word}_t$ is the target, which is equal to the input $x_t$ (we want to reproduce exactly $x_t$). The problem is that sending all the $x_i$ to the input embedding layer, there can be correlation with future words, in the sense that the algorithm can learn to predict the next word, using the next word instead of the present one. For this reason, within each decoder cell a causal mask mechanism is introduced, so that even if we really send all $(x_1,\\ldots)$ at the same time, only the 'not future ones' count:\n",
    "    - **causal mask mechanism**: like positional encoding, but to the scaled attention scores you add a 'look-ahed mask' which has -inf values for future tokens prediction and zero otherwise. Then the softmax kills the 'future' values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5388a9a5",
   "metadata": {
    "id": "5388a9a5"
   },
   "source": [
    "**GPT vs chat GPT**\n",
    "\n",
    "GPT (Generative Pretrained Transformer) is a decoder-only model because it is trained using a causal language modeling objective, where the goal is to predict the next token in a sequence given the previous tokens. During training, the input sequence is shifted to the right, and the model learns to generate output tokens autoregressively, one at a time. This process allows GPT to generate coherent and contextually relevant text based on the given input prompt.\n",
    "GPT is a family of large-scale transformer-based language models trained on diverse internet text data. GPT models are designed for a wide range of natural language processing tasks, such as text generation, translation, summarization, and question-answering. They generate responses based on the input text (prompt) but do not maintain a consistent conversation history.\n",
    "\n",
    "On the other hand, ChatGPT is a fine-tuned version of the GPT model, specifically designed for conversational AI applications. It is trained to maintain a consistent conversation history and generate contextually relevant responses, making it more suitable for chatbot-like interactions. ChatGPT excels at understanding and generating human-like dialogues, providing coherent and engaging responses in a conversational setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d602c700",
   "metadata": {
    "id": "d602c700"
   },
   "source": [
    "**SOURCE OF CONFUSION on GPT**\n",
    "\n",
    "A priori, one expects to use `nn.TransformerDecoderLayer` when constructing a decoder BUT `nn.TransformerDecoderLayer` is built to live in an encoder-decoder system, and for this reason it expects as input both self-attention (outputs of the decoder) AND cross-attention (outputs of the encoder). Thus:\n",
    "1. If we are building a lonely decoder, which uses self-attention and causal mask WITHOUT cross-attention, we should use `nn.TransformerEncoderLayer`WITH ALSO the specification of the causal mask (defined by hand);\n",
    "2. If we are building an encoder-decoder system, the decoder has a self-attention mechanism and a cross-attention mechanism. Here we use `nn.TransformerDecoderLayer`, which has (for the self-attention part) an integrated causal mask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07097d8",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b538938",
   "metadata": {},
   "source": [
    "It utilizes **entire sequences** simultaneously, **enhancing comprehension** of the semantic relations, using both sides of the target word (**bidirectional**). It is not used for text generation, but for predicting known words (text summarization, sentiment analysis, question answering). The idea is to mask some known words (15% of the text), or **MLM: Masked language Modelling**, where 'mask' must not be confused with the causal mask of decoder. In the 15%, 12% is replaced with <mask>, 1.5 with random token, and 1.5% remains unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4e1d51",
   "metadata": {},
   "source": [
    "Usally BERT is **PRETRAINED** using **MLM** and **NSP: Next Sentence Prediction** on vast corpora. The motivation behind unlabeled pretraining transformers is to address the limitations of traditional approaches that require significant amounts of labeled data for each specific task. Pretraining leverages the abundance of **unlabeled text data** available on the internet and facilitates transfer learning, where knowledge learned from one task can be transferred to aid in solving other related tasks.\n",
    "\n",
    "**NSP**\n",
    "\n",
    "This is a two classes (y=0/1) classification problem, where we take a complete sequence, we split it in two, take the first part as input and the second as target with y=1. Then give to the model other second parts sentences with y=0. In this way, training the model to the binary classification, BERT learns the logical relation between sentences, and not only among tokens. The complete sentence is modified adding <CLP> at the beginning and <SEP> at the end of the first (input) sentence.\n",
    "\n",
    "<span style=\"background-color: yellow\"> MLM: understand relations between words </span> and <span style=\"background-color: yellow\">NSP: enables the model to learn sentence-level relationships </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470a13cd",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7375239",
   "metadata": {},
   "source": [
    "It improves the standard language translation (using a RNN) by using enitre sequences at the same time, enhancing spped, efficiency adn understanding of contextual relationship of a word in a sentence, regardless of its lenght. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52acad84",
   "metadata": {
    "id": "52acad84"
   },
   "source": [
    "# 7) GPT-like Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dacf3bb",
   "metadata": {
    "id": "6dacf3bb"
   },
   "source": [
    "When training language models, it is generally advisable to use general-domain text. However, in this case, we are using the IMDB dataset, which is well-suited for classification tasks. Also, we use IMDB due to its smaller size and compatibility with machines that have limited RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "738564c9",
   "metadata": {
    "executionInfo": {
     "elapsed": 322948,
     "status": "aborted",
     "timestamp": 1752786900780,
     "user": {
      "displayName": "Alessio Fontanarossa",
      "userId": "15715623213071598062"
     },
     "user_tz": -120
    },
    "id": "738564c9"
   },
   "outputs": [],
   "source": [
    "#running time: 1m 36s\n",
    "\n",
    "from torchtext.datasets import IMDB # sentiment analysis dataset with (label, text) pairs\n",
    "\n",
    "train_iter, val_iter = IMDB()\n",
    "\n",
    "UNK_IDX, PAD_IDX, EOS_IDX = 0, 1, 2\n",
    "special_symbols = ['<unk>', '<pad>', '<|endoftext|>']\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "\n",
    "    for _,data_sample in data_iter:\n",
    "        yield  tokenizer(data_sample)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials = special_symbols, special_first = True)\n",
    "vocab.set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21019ab1",
   "metadata": {
    "id": "21019ab1"
   },
   "source": [
    "Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "22498323",
   "metadata": {
    "id": "22498323",
    "outputId": "9dbe56da-3002-4fca-a694-f5f34cbe24e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.')\n",
      "(1, '\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn\\'t matter what one\\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\\'t true. I\\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\\'re treated to the site of Vincent Gallo\\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\\'s bodies.')\n"
     ]
    }
   ],
   "source": [
    "train_iter, val_iter = IMDB()\n",
    "iterator = iter(train_iter)\n",
    "\n",
    "print(next(iterator))\n",
    "\n",
    "print(next(iterator))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174261f9",
   "metadata": {
    "id": "174261f9"
   },
   "source": [
    "The following function gives a string of token (importantly with the white spaces), given a list of indices as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9833fc12",
   "metadata": {
    "executionInfo": {
     "elapsed": 318344,
     "status": "aborted",
     "timestamp": 1752786900792,
     "user": {
      "displayName": "Alessio Fontanarossa",
      "userId": "15715623213071598062"
     },
     "user_tz": -120
    },
    "id": "9833fc12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk> <pad> <|endoftext|>'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def index_to_en(ids_list):\n",
    "    string=''\n",
    "    for idx in ids_list:\n",
    "        token = vocab.get_itos()[idx]\n",
    "        string = string + ' ' +token\n",
    "    return string.strip()\n",
    "\n",
    "# index_to_en = lambda seq_en: \" \".join([vocab.get_itos()[index] for index in seq_en]) #done by IBM\n",
    "\n",
    "index_to_en([0,1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ef17a3",
   "metadata": {
    "id": "14ef17a3"
   },
   "source": [
    "## Creating the src and tgt sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54e3491",
   "metadata": {
    "id": "f54e3491"
   },
   "source": [
    "To train a regressive model, we want to predict the next word. To this end, given a text, we want to take a sentence and then to take the same sentence but shifted of one position. For example, we want to create:\n",
    "\n",
    "```\n",
    "src:  ['around', 'a', 'young', 'swedish', 'drama', 'student', 'named', 'lena', 'who', 'wants']\n",
    "tgt:  ['a', 'young', 'swedish', 'drama', 'student', 'named', 'lena', 'who', 'wants', 'to']\n",
    "```\n",
    "\n",
    "so that wi give to the model $x_0$ 'around', and it predicts $\\text{word}_1$ which must be confronted with 'a':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3cfcb4db",
   "metadata": {
    "executionInfo": {
     "elapsed": 312015,
     "status": "aborted",
     "timestamp": 1752786900805,
     "user": {
      "displayName": "Alessio Fontanarossa",
      "userId": "15715623213071598062"
     },
     "user_tz": -120
    },
    "id": "3cfcb4db"
   },
   "outputs": [],
   "source": [
    "def get_sample(block_size, text): # the src_ and tgt_sequence will be of block_size lenght\n",
    "                                  # text is a tokenized text\n",
    "    sample_leg = len(text)\n",
    "    random_sample_stop = sample_leg - block_size\n",
    "\n",
    "    # Check if a random sample can be taken (if the text is longer than block_size)\n",
    "    if random_sample_stop >= 1:\n",
    "        # Randomly select a starting point for the sample\n",
    "        random_start = torch.randint(low=0, high = random_sample_stop, size=(1,)).item()\n",
    "        # Define the endpoint of the sample\n",
    "        stop = random_start + block_size\n",
    "\n",
    "        # Create the input and target sequences\n",
    "        src_sequence = text[random_start:stop]\n",
    "        tgt_sequence= text[random_start + 1:stop + 1]\n",
    "\n",
    "    # Handle the case where the text length is exactly equal or less the block size\n",
    "    elif random_sample_stop <= 0:\n",
    "        # Start from the beginning and use the entire text\n",
    "        random_start = 0\n",
    "        stop = sample_leg\n",
    "        src_sequence= text[random_start:stop]\n",
    "        tgt_sequence = text[random_start + 1:stop]\n",
    "        # Append an empty string to maintain sequence alignment\n",
    "        tgt_sequence.append( '<|endoftext|>')\n",
    "\n",
    "    return src_sequence, tgt_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625f5adb",
   "metadata": {
    "id": "625f5adb"
   },
   "source": [
    "example of usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3756a9bd",
   "metadata": {
    "executionInfo": {
     "elapsed": 312013,
     "status": "aborted",
     "timestamp": 1752786900806,
     "user": {
      "displayName": "Alessio Fontanarossa",
      "userId": "15715623213071598062"
     },
     "user_tz": -120
    },
    "id": "3756a9bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src:  ['comes', 'away', 'with', 'no', 'new', 'perspectives', '(', 'unless', 'one', 'comes', 'up', 'with', 'one', 'while', 'one', \"'\", 's', 'mind', 'wanders', ',', 'as', 'it', 'will', 'invariably', 'do', 'during', 'this', 'pointless', 'film', ')']\n",
      "tgt:  ['away', 'with', 'no', 'new', 'perspectives', '(', 'unless', 'one', 'comes', 'up', 'with', 'one', 'while', 'one', \"'\", 's', 'mind', 'wanders', ',', 'as', 'it', 'will', 'invariably', 'do', 'during', 'this', 'pointless', 'film', ')', '.']\n"
     ]
    }
   ],
   "source": [
    "sentiment, text = next(iterator)\n",
    "text = tokenizer(text)\n",
    "block_size=30\n",
    "src_sequences, tgt_sequence=get_sample(block_size, text)\n",
    "\n",
    "print(\"src: \",src_sequences)\n",
    "print(\"tgt: \",tgt_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7822ecf2",
   "metadata": {
    "id": "7822ecf2"
   },
   "source": [
    "## Collate function and Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1eee1154",
   "metadata": {
    "executionInfo": {
     "elapsed": 309674,
     "status": "aborted",
     "timestamp": 1752786900807,
     "user": {
      "displayName": "Alessio Fontanarossa",
      "userId": "15715623213071598062"
     },
     "user_tz": -120
    },
    "id": "1eee1154"
   },
   "outputs": [],
   "source": [
    "BLOCK_SIZE=30\n",
    "\n",
    "UNK_IDX, PAD_IDX, EOS_IDX = 0, 1, 2\n",
    "\n",
    "def collate_batch(batch):\n",
    "    src_batch, tgt_batch = [], [] #the sum of lists\n",
    "    for _sentiment,_textt in batch:\n",
    "\n",
    "      src_sequence, tgt_sequence = get_sample(BLOCK_SIZE, tokenizer(_textt)) #src_sequence, tgt_sequence are list of tokens\n",
    "\n",
    "      src_sequence=vocab(src_sequence) #converted to indices through the vocab\n",
    "      tgt_sequence=vocab(tgt_sequence) #converted to indices through the vocab\n",
    "\n",
    "      src_sequence = torch.tensor(src_sequence, dtype=torch.int64)\n",
    "      tgt_sequence = torch.tensor(tgt_sequence, dtype=torch.int64)\n",
    "\n",
    "      src_batch.append(src_sequence)\n",
    "      tgt_batch.append(tgt_sequence)\n",
    "\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value = PAD_IDX, batch_first = False)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value = PAD_IDX, batch_first = False)\n",
    "\n",
    "    return src_batch.to(device), tgt_batch.to(device)\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "train_iter, val_iter = IMDB()\n",
    "\n",
    "train_dataloader = DataLoader(train_iter, batch_size = BATCH_SIZE, shuffle = True, collate_fn = collate_batch)\n",
    "val_dataloader= DataLoader(val_iter , batch_size = BATCH_SIZE, shuffle = True, collate_fn = collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b901505",
   "metadata": {
    "id": "7b901505"
   },
   "source": [
    "## Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4eeba9d",
   "metadata": {
    "id": "a4eeba9d"
   },
   "source": [
    "1. The first line generates a square matrix with True in the upper triangular part (diagonal included!), and then transpose it, and then the .float() inserts 0 on the false. So at the end of the first line we have\n",
    "```\n",
    "tensor([[1., 0., 0.],\n",
    "        [1., 1., 0.],\n",
    "        [1., 1., 1.]])\n",
    "```\n",
    "2. The second line replace the 0 in mask with -inf and the 1 with 0, this getting\n",
    "```\n",
    "tensor([[0., -inf, -inf],\n",
    "        [0., 0., -inf],\n",
    "        [0., 0., 0.]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "8a522eea",
   "metadata": {
    "executionInfo": {
     "elapsed": 307585,
     "status": "aborted",
     "timestamp": 1752786900808,
     "user": {
      "displayName": "Alessio Fontanarossa",
      "userId": "15715623213071598062"
     },
     "user_tz": -120
    },
    "id": "8a522eea"
   },
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(size, device = device):\n",
    "    mask = (torch.triu(torch.ones((size, size), device=device)) == 1).transpose(0, 1).float() #\n",
    "    mask = mask.masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f23fe7",
   "metadata": {
    "id": "54f23fe7"
   },
   "source": [
    "Next, given a sequence coming from a loader (which has dimension ([BLOCK_SIZE, BATCH_SIZE])), we want to apply the mask to it. Thus we create a src_mask with dimension BLOCK_SIZE x BLOCK_SIZE (that will be passed directly in the model nn.TrasformerDecoderLayer, without adding manually this to the input) and also a src_padding_mask, because our sentences are padded to the right, and the padding must not influence the attention mechanism. For this reason, we will pass to the nn.TrasformerDecoderLayer src_padding_mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c9f1cd04",
   "metadata": {
    "executionInfo": {
     "elapsed": 307579,
     "status": "aborted",
     "timestamp": 1752786900809,
     "user": {
      "displayName": "Alessio Fontanarossa",
      "userId": "15715623213071598062"
     },
     "user_tz": -120
    },
    "id": "c9f1cd04"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 30])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_mask(src, device = device):\n",
    "    src_seq_len = src.shape[0] # =BLOCK_SIZE\n",
    "\n",
    "    src_mask = generate_square_subsequent_mask(src_seq_len)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "\n",
    "    return src_mask , src_padding_mask\n",
    "\n",
    "#usage example\n",
    "dataset = iter(train_dataloader)\n",
    "src, tgt = next(dataset)\n",
    "src_mask,src_padding_mask = create_mask(src)\n",
    "src_mask.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e763c89",
   "metadata": {
    "id": "8e763c89"
   },
   "source": [
    "## Positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb42369",
   "metadata": {
    "id": "ffb42369"
   },
   "source": [
    "Second version of the positional encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f84e3ad2",
   "metadata": {
    "executionInfo": {
     "elapsed": 304904,
     "status": "aborted",
     "timestamp": 1752786900809,
     "user": {
      "displayName": "Alessio Fontanarossa",
      "userId": "15715623213071598062"
     },
     "user_tz": -120
    },
    "id": "f84e3ad2"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269148fd",
   "metadata": {
    "id": "269148fd"
   },
   "source": [
    "## My GPT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba929dd",
   "metadata": {
    "id": "fba929dd"
   },
   "source": [
    "If confused by the presence of encoder layers, give a look to **SOURCE OF CONFUSION on GPT** in 6). The decoder function is exactly equal to the forward one. The difference is that the forward method is used automatically in training, and we will use the encoder function to do inference/text generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0db8887d",
   "metadata": {
    "executionInfo": {
     "elapsed": 302522,
     "status": "aborted",
     "timestamp": 1752786900812,
     "user": {
      "displayName": "Alessio Fontanarossa",
      "userId": "15715623213071598062"
     },
     "user_tz": -120
    },
    "id": "0db8887d"
   },
   "outputs": [],
   "source": [
    "class CustomGPTModel(nn.Module):\n",
    "    def __init__(self, embed_size, vocab_size, num_heads, num_of_decoder_layers, max_seq_len=500, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "\n",
    "        #input layer for embedding and sum with positional encoding\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = PositionalEncoding(embed_size, dropout = dropout)\n",
    "\n",
    "        # Remaining layers are part of the TransformerDecoder\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model = embed_size, nhead = num_heads, dropout = dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers = num_of_decoder_layers)\n",
    "\n",
    "        # output layer for prediction\n",
    "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "      for p in self.parameters():\n",
    "          if p.dim() > 1:\n",
    "              nn.init.xavier_uniform_(p)\n",
    "\n",
    "\n",
    "    def forward(self, x, src_mask = None, src_padding_mask = None):\n",
    "\n",
    "        if src_mask is None and src_padding_mask is None:\n",
    "\n",
    "            src_mask, src_padding_mask = create_mask(x)\n",
    "\n",
    "        seq_length = x.size(0)\n",
    "\n",
    "        # Add positional embeddings to the input embeddings\n",
    "        x = self.embed(x)* math.sqrt(self.embed_size) #src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "\n",
    "\n",
    "        output = self.transformer_encoder(x, mask=src_mask, src_key_padding_mask = src_padding_mask)\n",
    "        logits = self.lm_head(output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "55314268",
   "metadata": {
    "executionInfo": {
     "elapsed": 302522,
     "status": "aborted",
     "timestamp": 1752786900813,
     "user": {
      "displayName": "Alessio Fontanarossa",
      "userId": "15715623213071598062"
     },
     "user_tz": -120
    },
    "id": "55314268"
   },
   "outputs": [],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "emsize = 200  # embedding dimension\n",
    "nlayers = 2  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
    "nhead = 2  # number of heads in ``nn.MultiheadAttention``\n",
    "dropout = 0.2  # dropout probability\n",
    "\n",
    "model = CustomGPTModel(embed_size = emsize, num_heads = nhead,\n",
    "                        num_of_decoder_layers = nlayers, vocab_size = ntokens,\n",
    "                        dropout = dropout).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86783e5",
   "metadata": {
    "id": "c86783e5"
   },
   "source": [
    "## Text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8bdd4c",
   "metadata": {
    "id": "bb8bdd4c"
   },
   "source": [
    "The text generation needs a starting point, which is called prompt. This prompt should be tokenized, should not be None and should not be too long:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a0c27d63",
   "metadata": {
    "executionInfo": {
     "elapsed": 298754,
     "status": "aborted",
     "timestamp": 1752786900814,
     "user": {
      "displayName": "Alessio Fontanarossa",
      "userId": "15715623213071598062"
     },
     "user_tz": -120
    },
    "id": "a0c27d63"
   },
   "outputs": [],
   "source": [
    "def indicized_prompt(prompt, block_size=BLOCK_SIZE):\n",
    "    # Handle None prompt\n",
    "    while prompt is None:\n",
    "        prompt = input(\"Sorry, prompt cannot be empty. Please enter a valid prompt: \")\n",
    "\n",
    "    tokens = tokenizer(prompt)\n",
    "    number_of_tokens = len(tokens)\n",
    "\n",
    "    # Handle long prompts\n",
    "    if number_of_tokens > block_size:\n",
    "        tokens = tokens[-block_size:]  # Keep last block_size characters\n",
    "\n",
    "    prompt_indices = vocab(tokens)\n",
    "\n",
    "    prompt_encoded = torch.tensor(prompt_indices, dtype=torch.int64).reshape(-1, 1)\n",
    "    return prompt_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e4bd09",
   "metadata": {
    "id": "d0e4bd09"
   },
   "source": [
    "Usage example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f6eae958",
   "metadata": {
    "executionInfo": {
     "elapsed": 298751,
     "status": "aborted",
     "timestamp": 1752786900815,
     "user": {
      "displayName": "Alessio Fontanarossa",
      "userId": "15715623213071598062"
     },
     "user_tz": -120
    },
    "id": "f6eae958"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paterfamilias'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval() #to deactivate dropout\n",
    "prompt_encoded = indicized_prompt(\"This is a prompt to get model generate next words.\").to(device) #11 tokens\n",
    "logits = model(prompt_encoded, src_mask = None).to(device)\n",
    "logits.shape #torch.Size([11, 1, 100685])\n",
    "logits = logits.transpose(0, 1) #torch.Size([1, 11, 100685])\n",
    "logits.shape\n",
    "logit_preiction =logits[:,-1]\n",
    "_, next_word_index = torch.max(logit_preiction, dim=1)\n",
    "next_word_index\n",
    "\n",
    "index_to_en(next_word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de03f9b5",
   "metadata": {
    "id": "de03f9b5"
   },
   "source": [
    "Then we can define a function that, given the indicized_prompt, do generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2db8727d",
   "metadata": {
    "executionInfo": {
     "elapsed": 298751,
     "status": "aborted",
     "timestamp": 1752786900816,
     "user": {
      "displayName": "Alessio Fontanarossa",
      "userId": "15715623213071598062"
     },
     "user_tz": -120
    },
    "id": "2db8727d"
   },
   "outputs": [],
   "source": [
    "def generate(model, prompt=None, max_new_tokens = 500, block_size = BLOCK_SIZE, vocab = vocab, tokenizer = tokenizer):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    prompt_encoded = indicized_prompt(prompt).to(device)\n",
    "    tokens = []\n",
    "\n",
    "    # Generate new tokens up to max_new_tokens\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Decode the encoded prompt using the model's decoder\n",
    "        logits = model(prompt_encoded,src_mask=None,src_padding_mask=None)\n",
    "\n",
    "        # Transpose the logits to bring the sequence length to the first dimension\n",
    "        logits = logits.transpose(0, 1)\n",
    "\n",
    "        # Select the logits of the last token in the sequence\n",
    "        logit_prediction = logits[:, -1]\n",
    "\n",
    "        # Choose the most probable next token from the logits(greedy decoding)\n",
    "        next_token_encoded = torch.argmax(logit_prediction, dim=-1).reshape(-1, 1)\n",
    "\n",
    "        # If the next token is the end-of-sequence (EOS) token, stop generation\n",
    "        if next_token_encoded.item() == EOS_IDX:\n",
    "            break\n",
    "\n",
    "        # Append the next token to the prompt_encoded and keep only the last 'block_size' tokens\n",
    "        prompt_encoded = torch.cat((prompt_encoded, next_token_encoded), dim=0)[-block_size:]\n",
    "\n",
    "        # Convert the next token index to a token string using the vocabulary\n",
    "        # Move the tensor back to CPU for vocab lookup if needed\n",
    "        token_id = next_token_encoded.to('cpu').item()\n",
    "        tokens.append(vocab.get_itos()[token_id])\n",
    "\n",
    "    # Join the generated tokens into a single string and return\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a84d2f0",
   "metadata": {
    "id": "0a84d2f0"
   },
   "source": [
    "The generation, of course, is awful because we did not train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "581f0822",
   "metadata": {
    "executionInfo": {
     "elapsed": 298751,
     "status": "aborted",
     "timestamp": 1752786900817,
     "user": {
      "displayName": "Alessio Fontanarossa",
      "userId": "15715623213071598062"
     },
     "user_tz": -120
    },
    "id": "581f0822"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pseuds bulb-lights bulb-lights bulb-lights bulb-lights enyard enyard paterfamilias graduating pain-in-the-butt pain-in-the-butt calista pain-in-the-butt pain-in-the-butt pain-in-the-butt early-century $23 $23 spivs predicable calista calista calista calista calista calista calista calista calista calista'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model,prompt=\"this is the beginning of\",max_new_tokens=30,vocab=vocab,tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91af6c82",
   "metadata": {
    "id": "91af6c82"
   },
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c1caf36f",
   "metadata": {
    "executionInfo": {
     "elapsed": 296196,
     "status": "aborted",
     "timestamp": 1752786900819,
     "user": {
      "displayName": "Alessio Fontanarossa",
      "userId": "15715623213071598062"
     },
     "user_tz": -120
    },
    "id": "c1caf36f"
   },
   "outputs": [],
   "source": [
    "loss_fn = CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "def evaluate(model: nn.Module, eval_data) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for src,tgt in eval_data:\n",
    "            tgt = tgt.to(device)\n",
    "            #seq_len = src.size(0)\n",
    "            logits = model(src)\n",
    "            total_loss +=  loss_fn(logits.reshape(-1, logits.shape[-1]), tgt.reshape(-1)).item()\n",
    "    return total_loss / (len(list(eval_data)) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "76236c48",
   "metadata": {
    "executionInfo": {
     "elapsed": 296197,
     "status": "aborted",
     "timestamp": 1752786900820,
     "user": {
      "displayName": "Alessio Fontanarossa",
      "userId": "15715623213071598062"
     },
     "user_tz": -120
    },
    "id": "76236c48"
   },
   "outputs": [],
   "source": [
    "#evaluate(model,val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4768d7da",
   "metadata": {
    "id": "4768d7da"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "64caf9e3",
   "metadata": {
    "id": "64caf9e3"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10000, gamma=0.9)\n",
    "\n",
    "def train(model: nn.Module,train_data) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 10000\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(list(train_data)) //BLOCK_SIZE\n",
    "    for batch,srctgt in enumerate(train_data):\n",
    "        src= srctgt[0]\n",
    "        tgt= srctgt[1]\n",
    "        logits = model(src,src_mask=None)\n",
    "        logits_flat = logits.reshape(-1, logits.shape[-1])\n",
    "        loss = loss_fn(logits_flat, tgt.reshape(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (batch % log_interval == 0 and batch > 0) or batch==42060:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            #cur_loss = total_loss / log_interval\n",
    "            cur_loss = total_loss / batch\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch//BLOCK_SIZE:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.4f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            start_time = time.time()\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e72abe5",
   "metadata": {},
   "source": [
    "It works, but it is commented for time saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1517fc6d",
   "metadata": {
    "id": "1517fc6d",
    "outputId": "53eeb7df-64b3-4169-ac5d-fdb082685205"
   },
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 1\n",
    "Train_losses= []\n",
    "Val_losses = []\n",
    "# for epoch in tqdm(range(1, epochs + 1)):\n",
    "#     epoch_start_time = time.time()\n",
    "#     train_loss = train(model,train_dataloader)\n",
    "#     val_loss = evaluate(model, val_dataloader)\n",
    "#     val_ppl = math.exp(val_loss)\n",
    "#     Train_losses.append(train_loss)\n",
    "#     Val_losses.append(val_loss)\n",
    "\n",
    "#     elapsed = time.time() - epoch_start_time\n",
    "#     print('-' * 89)\n",
    "#     print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "#         f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "#     print('-' * 89)\n",
    "\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         torch.save(model.state_dict(), 'model_best_val_loss.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "9966c057",
   "metadata": {
    "id": "9966c057"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate the number of epochs (assuming the lengths of train_losses and val_losses are equal)\n",
    "num_epochs = len(Train_losses)\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the training losses\n",
    "ax.plot(range(num_epochs), Train_losses, label='Training Loss', color='blue')\n",
    "\n",
    "# Plot the validation losses\n",
    "ax.plot(range(num_epochs), Val_losses, label='Validation Loss', color='orange')\n",
    "\n",
    "# Set the x-axis label\n",
    "ax.set_xlabel('Epoch')\n",
    "\n",
    "# Set the y-axis label\n",
    "ax.set_ylabel('Loss')\n",
    "\n",
    "# Set the title of the plot\n",
    "ax.set_title('Training and Validation Losses')\n",
    "\n",
    "# Add a legend to the plot\n",
    "ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdfa96a",
   "metadata": {},
   "source": [
    "## Using Hugging-Face GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f1c5bc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I do not love my family\n",
      "Generated Text: I do not love my family, but I love my family. I love\n"
     ]
    }
   ],
   "source": [
    "tokenizer1 = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "input_text = \"I do not love my family\"\n",
    "\n",
    "# Tokenize the input text and prepare the input for the model\n",
    "input_ids = tokenizer1.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text using the model\n",
    "# Set the desired length of the generated text (max_length),\n",
    "# and other generation parameters like temperature, top_k, and top_p\n",
    "max_length = 15\n",
    "temperature = 0.7\n",
    "top_k = 50\n",
    "top_p = 0.95\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=max_length,\n",
    "    temperature=temperature,\n",
    "    top_k=top_k,\n",
    "    top_p=top_p,\n",
    "    pad_token_id=tokenizer1.eos_token_id,\n",
    ")\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer1.decode(generated_ids[0], skip_special_tokens = True)\n",
    "\n",
    "# Print the input prompt and the generated text\n",
    "print(f\"Input: {input_text}\")\n",
    "print(f\"Generated Text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2305cd",
   "metadata": {},
   "source": [
    "# 8) BERT-like Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b13460",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4186a6f",
   "metadata": {},
   "source": [
    "We start with the usual things, as in 7), but with more special symbols:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a9c2afe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 125092\n"
     ]
    }
   ],
   "source": [
    "from torchtext.datasets import IMDB\n",
    "\n",
    "train_iter, test_iter = IMDB(split=('train', 'test'))\n",
    "all_data_iter = chain(train_iter, test_iter)\n",
    "\n",
    "PAD_IDX,CLS_IDX, SEP_IDX,  MASK_IDX,UNK_IDX= 0, 1, 2, 3, 4\n",
    "special_symbols = ['[PAD]','[CLS]', '[SEP]','[MASK]','[UNK]']\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "\n",
    "    for label, data_sample in data_iter:\n",
    "        yield  tokenizer(data_sample)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(all_data_iter), specials = special_symbols, special_first = True) #build vocab from all data\n",
    "vocab.set_default_index(UNK_IDX)\n",
    "\n",
    "VOCAB_SIZE=len(vocab)\n",
    "print('Size of vocabulary:', VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "524779cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'would', 'put', 'this', 'at', 'the', 'top', 'of', 'my', 'list', 'of', 'films', 'in', 'the', 'category', 'of', 'unwatchable', 'trash', '!', 'there']\n"
     ]
    }
   ],
   "source": [
    "def index_to_en(ids_list): #index_to_english\n",
    "    string=''\n",
    "    for idx in ids_list:\n",
    "        token = vocab.get_itos()[idx]\n",
    "        string = string + ' ' +token\n",
    "    return string.strip()\n",
    "\n",
    "all_data_iter = chain(train_iter, test_iter)\n",
    "\n",
    "fifth_item_tokens = next(islice(yield_tokens(all_data_iter), 5, None))\n",
    "print(fifth_item_tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86822607",
   "metadata": {},
   "source": [
    "Then we want to prepare data for MLM and NSP. We define a MLM and a NSP function, which at the end will be unified. Notice that MLM is used also in NSP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada6d3c5",
   "metadata": {},
   "source": [
    "### MLM (Masked Language Modelling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcc9821",
   "metadata": {},
   "source": [
    "The following functions decide whether each token in a sequence should be masked, left unchanged, or replaced with a random token. This process is essential for training the model to predict masked words based on their context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7362e63b",
   "metadata": {},
   "source": [
    "We want to achieve something like this\n",
    "```\n",
    "original_input is:  'The sun sets behind the distant mountains'\n",
    "BERT_input is:  [['[MASK]','sun','sets','behind','the','[MASK]','mountains','.']]\n",
    "BERT_label is:  [['the','[PAD]','[PAD]','[PAD]','[PAD]','casual token','[PAD]','[PAD]']]\n",
    "```\n",
    "and in particular\n",
    "\n",
    "- If the token is not substituted the corresponding label is [PAD]:\n",
    "- If it is masked, the token is substituted with [MASK] there are two possibilities:\n",
    "    1. Or the label is the original token:\n",
    "    2. Or with a casual token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1603311",
   "metadata": {},
   "source": [
    "So we prepare a function for masking a single token, and then apply to a whole tokenized sequence in the subsequent function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c25ba7f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bernoulli_true_false(probability):\n",
    "    # Create a Bernoulli distribution with probability p\n",
    "    bernoulli_dist = torch.distributions.Bernoulli(torch.tensor([probability]))\n",
    "    # Sample from this distribution and convert 1 to True and 0 to False\n",
    "    return bernoulli_dist.sample().item() == 1\n",
    "\n",
    "bernoulli_true_false(0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e82dcaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Masking(token):\n",
    "    \n",
    "    mask = bernoulli_true_false(0.2) # Decide whether to mask this token (20% chance); this is True or False\n",
    "\n",
    "    if mask is False: # If mask is False, return with '[PAD]' label\n",
    "        token_ = token\n",
    "        mask_label = '[PAD]'\n",
    "\n",
    "    random_opp = bernoulli_true_false(0.5)\n",
    "    random_swich = bernoulli_true_false(0.5)\n",
    "\n",
    "    if mask is True and random_opp is True and random_swich is True: # Replace the token with '[MASK]' and set label to a random token\n",
    "        token_ = '[MASK]'\n",
    "        mask_label = index_to_en(torch.randint(0, VOCAB_SIZE, (1,)))\n",
    "    \n",
    "    elif mask is True and random_opp is True and random_swich is False: # Leave the token unchanged and set label to the same token\n",
    "        token_ = token\n",
    "        mask_label = token\n",
    "\n",
    "    else: # Replace the token with '[MASK]' and set label to the original token\n",
    "        token_ = '[MASK]'\n",
    "        mask_label = token\n",
    "\n",
    "    return token_, mask_label\n",
    "\n",
    "def BERT_Masking(token):\n",
    "    # Decide if this token should be masked (15% chance)\n",
    "    apply_mask = bernoulli_true_false(0.15)\n",
    "\n",
    "    if not apply_mask:\n",
    "        return token, '[PAD]'\n",
    "\n",
    "    # Choose the masking strategy\n",
    "    rand = torch.rand(1).item()\n",
    "\n",
    "    if rand < 0.8:\n",
    "        # 80% of the time â†’ [MASK] in input, original token in label\n",
    "        return '[MASK]', token\n",
    "    elif rand < 0.9:\n",
    "        # 10% of the time â†’ original token in input, original token in label\n",
    "        return token, token\n",
    "    else:\n",
    "        # 10% of the time â†’ random token in input, original token in label\n",
    "        random_index = torch.randint(0, VOCAB_SIZE, (1,))\n",
    "        return index_to_en(random_index), token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "719e7ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_mlm(tokens, include_raw_tokens = False):\n",
    "    raw_tokens_list = []  # List to store raw tokens if needed\n",
    "    bert_input = []  # List to store sentences processed for BERT's MLM\n",
    "    bert_label = []  # List to store labels for each token (mask, random, or unchanged)\n",
    "    current_raw_tokens = []\n",
    "    current_bert_input = []\n",
    "    current_bert_label = []\n",
    "\n",
    "    for token in tokens:\n",
    "        masked_token, mask_label = Masking(token)\n",
    "        masked_token, mask_label = BERT_Masking(token)\n",
    "\n",
    "        current_bert_input.append(masked_token)\n",
    "        current_bert_label.append(mask_label)\n",
    "\n",
    "        if include_raw_tokens is True:\n",
    "            current_raw_tokens.append(token)\n",
    "\n",
    "        # Check if the token is a sentence delimiter (., ?, !)\n",
    "        if token in ['.', '?', '!']:\n",
    "            # If current sentence has more than two tokens, consider it a valid sentence\n",
    "            if len(current_bert_input) > 2:\n",
    "                bert_input.append(current_bert_input)\n",
    "                bert_label.append(current_bert_label)\n",
    "                # If including raw tokens, add the current list of raw tokens to the raw tokens list\n",
    "                if include_raw_tokens:\n",
    "                    raw_tokens_list.append(current_raw_tokens)\n",
    "\n",
    "                # Reset the lists for the next sentence\n",
    "                current_bert_input = []\n",
    "                current_bert_label = []\n",
    "                current_raw_tokens = []\n",
    "            else:\n",
    "                # If the current sentence is too short, discard it and reset lists\n",
    "                current_bert_input = []\n",
    "                current_bert_label = []\n",
    "                current_raw_tokens = []\n",
    "\n",
    "    # Add any remaining tokens as a sentence if there are any\n",
    "    if current_bert_input:\n",
    "        bert_input.append(current_bert_input)\n",
    "        bert_label.append(current_bert_label)\n",
    "        if include_raw_tokens:\n",
    "            raw_tokens_list.append(current_raw_tokens)\n",
    "\n",
    "    # Return the prepared lists for BERT's MLM training\n",
    "    return (bert_input, bert_label, raw_tokens_list) if include_raw_tokens else (bert_input, bert_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8b57d9",
   "metadata": {},
   "source": [
    "Usage example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "5e142f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw: [['the', 'sun', 'sets', 'behind', 'the', 'distant', 'mountains', '.']]\n",
      "bert_input is: [['the', 'sun', '[MASK]', 'behind', '[MASK]', 'distant', 'mountains', '.']]\n",
      "bert_label is: [['[PAD]', '[PAD]', 'sets', '[PAD]', 'the', '[PAD]', '[PAD]', '[PAD]']]\n"
     ]
    }
   ],
   "source": [
    "original_input = \"The sun sets behind the distant mountains.\"\n",
    "tokens = tokenizer(original_input)\n",
    "\n",
    "bert_input, bert_label, raw_tokens_list = prepare_for_mlm(tokens, include_raw_tokens = True)\n",
    "\n",
    "print('raw:',raw_tokens_list)\n",
    "print('bert_input is:',bert_input)\n",
    "print('bert_label is:',bert_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cda79f8",
   "metadata": {},
   "source": [
    "### NSP (Next Sequence Predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23371ac",
   "metadata": {},
   "source": [
    "We want to achieve something like this\n",
    "```\n",
    "BERT Input:\n",
    "[['[CLS]', 'she', 'enjoys', 'reading', 'books', '[SEP]'], ['he', 'likes', 'playing', 'guitar', '[SEP]']]\n",
    "BERT Label:\n",
    "[['[PAD]', '[PAD]', '[PAD]', '[PAD]', 'books', '[PAD]'], ['[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']]\n",
    "Is Next:  [1]\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "BERT Input:\n",
    "[['[CLS]', 'he', 'likes', 'playing', 'guitar', '[SEP]'], ['i', 'love', 'apples', '[SEP]']]\n",
    "BERT Label:\n",
    "[['[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[PAD]', 'love', '[PAD]', '[PAD]']]\n",
    "Is Next:  [0]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "2ab07828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_for_nsp(input_sentences, input_masked_labels): # input_sentences = list of tokenized sentences\n",
    "                                                            # input_masked_labels = corresponding label masked list \n",
    "\n",
    "    if len(input_sentences) < 2:\n",
    "       raise ValueError(\"Must have at least two sentences.\")\n",
    "\n",
    "\n",
    "    # Verify that both input lists are of the same length and have a sufficient number of sentences\n",
    "    if len(input_sentences) != len(input_masked_labels):\n",
    "        raise ValueError(\"Both lists must have the same number of items.\")\n",
    "\n",
    "    bert_input = []\n",
    "    bert_label = []\n",
    "    is_next = []\n",
    "\n",
    "    available_indices = list(range(len(input_sentences)))\n",
    "\n",
    "    while len(available_indices) >= 2:\n",
    "        if random.random() < 0.5:\n",
    "            # Choose two consecutive sentences to simulate the 'next sentence' scenario\n",
    "            index = random.choice(available_indices[:-1])  # Exclude the last index\n",
    "            # append list and add  '[CLS]' and  '[SEP]' tokens\n",
    "            bert_input.append([['[CLS]']+input_sentences[index]+ ['[SEP]'],input_sentences[index + 1]+ ['[SEP]']])\n",
    "            bert_label.append([['[PAD]']+input_masked_labels[index]+['[PAD]'], input_masked_labels[index + 1]+ ['[PAD]']])\n",
    "            is_next.append(1)  # Label 1 indicates these sentences are consecutive\n",
    "\n",
    "            # Remove the used indices\n",
    "            available_indices.remove(index)\n",
    "            if index + 1 in available_indices:\n",
    "                available_indices.remove(index + 1)\n",
    "        else:\n",
    "            # Choose two random distinct sentences to simulate the 'not next sentence' scenario\n",
    "            indices = random.sample(available_indices, 2)\n",
    "            bert_input.append([['[CLS]']+input_sentences[indices[0]]+['[SEP]'],input_sentences[indices[1]]+ ['[SEP]']])\n",
    "            bert_label.append([['[PAD]']+input_masked_labels[indices[0]]+['[PAD]'], input_masked_labels[indices[1]]+['[PAD]']])\n",
    "            is_next.append(0)  # Label 0 indicates these sentences are not consecutive\n",
    "\n",
    "            # Remove the used indices\n",
    "            available_indices.remove(indices[0])\n",
    "            available_indices.remove(indices[1])\n",
    "\n",
    "\n",
    "\n",
    "    return bert_input, bert_label, is_next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f32c577",
   "metadata": {},
   "source": [
    "Usage example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5e892f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Input:\n",
      "[['[CLS]', 'she', 'enjoys', 'reading', 'books', '[SEP]'], ['he', 'likes', 'playing', 'guitar', '[SEP]']]\n",
      "BERT Label:\n",
      "[['[PAD]', '[PAD]', '[PAD]', '[PAD]', 'books', '[PAD]'], ['[PAD]', '[PAD]', '[PAD]', 'guitar', '[PAD]']]\n",
      "Is Next:  [1]\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "BERT Input:\n",
      "[['[CLS]', 'he', 'likes', 'playing', 'guitar', '[SEP]'], ['i', 'love', 'apples', '[SEP]']]\n",
      "BERT Label:\n",
      "[['[PAD]', '[PAD]', '[PAD]', '[PAD]', 'guitar', '[PAD]'], ['[PAD]', '[PAD]', '[PAD]', '[PAD]']]\n",
      "Is Next:  [0]\n"
     ]
    }
   ],
   "source": [
    "#flatten the tensor\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "# Sample input sentences\n",
    "input_sentences = [[\"i\", \"love\", \"apples\"], [\"she\", \"enjoys\", \"reading\", \"books\"], [\"he\", \"likes\", \"playing\", \"guitar\"]]\n",
    "# Create masked labels for the sentences\n",
    "input_masked_labels=[]\n",
    "for sentence in input_sentences:\n",
    "  _, current_masked_label= prepare_for_mlm(sentence, include_raw_tokens = False)\n",
    "  input_masked_labels.append(flatten(current_masked_label))\n",
    "# Create NSP pairs and labels\n",
    "random.seed(100)\n",
    "bert_input, bert_label, is_next = process_for_nsp(input_sentences, input_masked_labels)\n",
    "\n",
    "# Print the output\n",
    "print(\"BERT Input:\")\n",
    "for pair in bert_input:\n",
    "    print(pair)\n",
    "print(\"BERT Label:\")\n",
    "for pair in bert_label:\n",
    "    print(pair)\n",
    "print(\"Is Next: \", is_next)\n",
    "print(\"-\"*200)\n",
    "random.seed(1000)\n",
    "bert_input, bert_label, is_next = process_for_nsp(input_sentences, input_masked_labels)\n",
    "\n",
    "# Print the output\n",
    "print(\"BERT Input:\")\n",
    "for pair in bert_input:\n",
    "    print(pair)\n",
    "print(\"BERT Label:\")\n",
    "for pair in bert_label:\n",
    "    print(pair)\n",
    "print(\"Is Next: \", is_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3077de",
   "metadata": {},
   "source": [
    "### Final preparation and dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ce5d3f",
   "metadata": {},
   "source": [
    "`prepare_bert_final_inputs` consolidates the prepared data for MLM and NSP into a format suitable for BERT training, including converting tokens to indices, padding sequences for uniform length, and generating segment labels to distinguish between pairs of sentences. This function is the final step in preparing data for BERT, ensuring it is in the correct format for effective model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "ac88cd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_bert_final_inputs(bert_inputs, bert_labels, is_nexts,to_tenor=True):\n",
    "    \"\"\"\n",
    "    Prepare the final input lists for BERT training.\n",
    "    \"\"\"\n",
    "    def zero_pad_list_pair(pair_, pad='[PAD]'):\n",
    "        pair=deepcopy(pair_)\n",
    "        max_len = max(len(pair[0]), len(pair[1]))\n",
    "        #append [PAD] to each sentence in the pair till the maximum length reaches\n",
    "        pair[0].extend([pad] * (max_len - len(pair[0])))\n",
    "        pair[1].extend([pad] * (max_len - len(pair[1])))\n",
    "        return pair[0], pair[1]\n",
    "\n",
    "    #flatten the tensor\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    #transform tokens to vocab indices\n",
    "    tokens_to_index=lambda tokens: [vocab[token] for token in tokens]\n",
    "\n",
    "    bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_final = [], [], [], []\n",
    "\n",
    "    for bert_input, bert_label,is_next in zip(bert_inputs, bert_labels,is_nexts):\n",
    "        # Create segment labels for each pair of sentences\n",
    "        segment_label = [[1] * len(bert_input[0]), [2] * len(bert_input[1])]\n",
    "\n",
    "        # Zero-pad the bert_input and bert_label and segment_label\n",
    "        bert_input_padded = zero_pad_list_pair(bert_input)\n",
    "        bert_label_padded = zero_pad_list_pair(bert_label)\n",
    "        segment_label_padded = zero_pad_list_pair(segment_label,pad=0)\n",
    "\n",
    "        #convert to tensors\n",
    "        if to_tenor:\n",
    "\n",
    "            # Flatten the padded inputs and labels, transform tokens to their corresponding vocab indices, and convert them to tensors\n",
    "            bert_inputs_final.append(torch.tensor(tokens_to_index(flatten(bert_input_padded)),dtype=torch.int64))\n",
    "            #bert_labels_final.append(torch.tensor(tokens_to_index(flatten(bert_label_padded)),dtype=torch.int64))\n",
    "            bert_labels_final.append(torch.tensor(tokens_to_index(flatten(bert_label_padded)),dtype=torch.int64))\n",
    "            segment_labels_final.append(torch.tensor(flatten(segment_label_padded),dtype=torch.int64))\n",
    "            is_nexts_final.append(is_next)\n",
    "\n",
    "        else:\n",
    "          # Flatten the padded inputs and labels\n",
    "            bert_inputs_final.append(flatten(bert_input_padded))\n",
    "            bert_labels_final.append(flatten(bert_label_padded))\n",
    "            segment_labels_final.append(flatten(segment_label_padded))\n",
    "            is_nexts_final.append(is_next)\n",
    "\n",
    "    return bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "56f893f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\t\t [[['[CLS]', 'he', 'likes', 'playing', 'guitar', '[SEP]'], ['i', 'love', 'apples', '[SEP]']]] \n",
      "inputs_final:\t [tensor([    1,    35,  1172,   409,  5184,     2,    16,   138, 14585,     2,     0,     0])] \n",
      "bert labels final:\t [tensor([   0,    0,    0,    0, 5184,    0,    0,    0,    0,    0,    0,    0])] \n",
      "segment labels final:\t [tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 0, 0])] \n",
      "is nexts final:\t [0]\n"
     ]
    }
   ],
   "source": [
    "bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_final=prepare_bert_final_inputs(bert_input, bert_label, is_next,to_tenor=True)\n",
    "torch.set_printoptions(linewidth=10000)# this assures that whole output is printed in one line\n",
    "print(\"input:\\t\\t\",bert_input,\"\\ninputs_final:\\t\",bert_inputs_final,\"\\nbert labels final:\\t\",bert_labels_final,\"\\nsegment labels final:\\t\",segment_labels_final,\"\\nis nexts final:\\t\",is_nexts_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c31a18",
   "metadata": {},
   "source": [
    "Then we create a csv final which contains all this. This will be the dataset for training the BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa30ae3",
   "metadata": {},
   "source": [
    "Run one time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "00a91bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_file_path ='train_bert_data_new.csv'\n",
    "# with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "#     csv_writer = csv.writer(file)\n",
    "#     csv_writer.writerow(['Original Text', 'BERT Input', 'BERT Label', 'Segment Label', 'Is Next'])\n",
    "\n",
    "#     # Wrap train_iter with tqdm for a progress bar\n",
    "#     for n, (_, sample) in enumerate(tqdm(train_iter, desc=\"Processing samples\")):\n",
    "#         # Tokenize the sample input\n",
    "#         tokens = tokenizer(sample)\n",
    "#         # Create MLM inputs and labels\n",
    "#         bert_input, bert_label = prepare_for_mlm(tokens, include_raw_tokens = False)\n",
    "#         if len(bert_input) < 2:\n",
    "#             continue\n",
    "#         # Create NSP pairs, token labels, and is_next label\n",
    "#         bert_inputs, bert_labels, is_nexts = process_for_nsp(bert_input, bert_label)\n",
    "#         # add zero-paddings, map tokens to vocab indices and create segment labels\n",
    "#         bert_inputs, bert_labels, segment_labels, is_nexts = prepare_bert_final_inputs(bert_inputs, bert_labels, is_nexts)\n",
    "#         # convert tensors to lists, convert lists to JSON-formatted strings\n",
    "#         for bert_input, bert_label, segment_label, is_next in zip(bert_inputs, bert_labels, segment_labels, is_nexts):\n",
    "#             bert_input_str = json.dumps(bert_input.tolist())\n",
    "#             bert_label_str = json.dumps(bert_label.tolist())\n",
    "#             segment_label_str = ','.join(map(str, segment_label.tolist()))\n",
    "#             # Write the data to a CSV file row-by-row\n",
    "#             csv_writer.writerow([sample, bert_input_str, bert_label_str, segment_label_str, is_next])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f573ce",
   "metadata": {},
   "source": [
    "## Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "5f42e968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-28 10:47:57--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/bZaoQD52DcMpE7-kxwAG8A.zip\n",
      "Risoluzione di cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 198.23.119.245\n",
      "Connessione a cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|198.23.119.245|:443... connesso.\n",
      "Richiesta HTTP inviata, in attesa di risposta... 200 OK\n",
      "Lunghezza: 88958506 (85M) [application/zip]\n",
      "Salvataggio in: Â«BERT_dataset.zipÂ»\n",
      "\n",
      "BERT_dataset.zip    100%[===================>]  84.84M  1.09MB/s    in 39s     \n",
      "\n",
      "2025-07-28 10:48:37 (2.20 MB/s) - Â«BERT_dataset.zipÂ» salvato [88958506/88958506]\n",
      "\n",
      "UnZip 6.00 of 20 April 2009, by Info-ZIP, with modifications by Apple Inc.\n",
      "Please report bugs to Apple.\n",
      "\n",
      "Usage: unzip [-Z] [-opts[modifiers]] file[.zip] [list] [-x xlist] [-d exdir]\n",
      "  Default action is to extract files in list, except those in xlist, to exdir;\n",
      "  file[.zip] may be a wildcard.  -Z => ZipInfo mode (\"unzip -Z\" for usage).\n",
      "\n",
      "  -p  extract files to pipe, no messages     -l  list files (short format)\n",
      "  -f  freshen existing files, create none    -t  test compressed archive data\n",
      "  -u  update files, create if necessary      -z  display archive comment only\n",
      "  -v  list verbosely/show version info       -T  timestamp archive to latest\n",
      "  -x  exclude files that follow (in xlist)   -d  extract files into exdir\n",
      "modifiers:\n",
      "  -n  never overwrite existing files         -q  quiet mode (-qq => quieter)\n",
      "  -o  overwrite files WITHOUT prompting      -a  auto-convert any text files\n",
      "  -j  junk paths (do not make directories)   -aa treat ALL files as text\n",
      "  -C  match filenames case-insensitively     -L  make (some) names lowercase\n",
      "  -X  restore UID/GID info                   -V  retain VMS version numbers\n",
      "  -K  keep setuid/setgid/tacky permissions   -M  pipe through \"more\" pager\n",
      "See \"unzip -hh\" or unzip.txt for more help.  Examples:\n",
      "  unzip data1 -x joe   => extract all files except joe from zipfile data1.zip\n",
      "  unzip -p foo | more  => send contents of foo.zip via pipe into program more\n",
      "  unzip -fo foo ReadMe => quietly replace existing ReadMe if archive file newer\n"
     ]
    }
   ],
   "source": [
    "!wget -O BERT_dataset.zip https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/bZaoQD52DcMpE7-kxwAG8A.zip\n",
    "!unzip -y BERT_dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ba1ed3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTCSVDataset(Dataset):\n",
    "    def __init__(self, filename):\n",
    "        self.data = pd.read_csv(filename)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        try:\n",
    "            \n",
    "            bert_input = torch.tensor(json.loads(row['BERT Input']), dtype=torch.long)\n",
    "            bert_label = torch.tensor(json.loads(row['BERT Label']), dtype=torch.long)\n",
    "            segment_label = torch.tensor([int(x) for x in row['Segment Label'].split(',')], dtype=torch.long)\n",
    "            is_next = torch.tensor(row['Is Next'], dtype=torch.long)\n",
    "            original_text = row['Original Text']  # If you want to use it\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON for row {idx}: {e}\")\n",
    "            print(\"BERT Input:\", row['BERT Input'])\n",
    "            print(\"BERT Label:\", row['BERT Label'])\n",
    "            # Handle the error, e.g., by skipping this row or using default values\n",
    "            return None  # or some default values\n",
    "\n",
    "            # Tokenizing the original text with BERT\n",
    "        encoded_input = self.tokenizer.encode_plus(\n",
    "            original_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = encoded_input['input_ids'].squeeze()\n",
    "        attention_mask = encoded_input['attention_mask'].squeeze()\n",
    "\n",
    "        return(bert_input, bert_label, segment_label, is_next, input_ids, attention_mask, original_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fca2f43",
   "metadata": {},
   "source": [
    "### Collate function and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "4f4ca7cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './bert_dataset/bert_train_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[166], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m train_dataset_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./bert_dataset/bert_train_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     28\u001b[0m test_dataset_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./bert_dataset/bert_test_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 30\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mBERTCSVDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m BERTCSVDataset(test_dataset_path)\n\u001b[1;32m     33\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_batch)\n",
      "Cell \u001b[0;32mIn[165], line 3\u001b[0m, in \u001b[0;36mBERTCSVDataset.__init__\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/Trans_env/lib/python3.10/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/Trans_env/lib/python3.10/site-packages/pandas/io/parsers/readers.py:586\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    571\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    572\u001b[0m     dialect,\n\u001b[1;32m    573\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    583\u001b[0m )\n\u001b[1;32m    584\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/Trans_env/lib/python3.10/site-packages/pandas/io/parsers/readers.py:482\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    479\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    481\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/miniconda3/envs/Trans_env/lib/python3.10/site-packages/pandas/io/parsers/readers.py:811\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwds:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 811\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/Trans_env/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1040\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1037\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown engine: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (valid options are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1038\u001b[0m     )\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;66;03m# error: Too many arguments for \"ParserBase\"\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/Trans_env/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:51\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     48\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musecols\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musecols\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# open handles\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open_handles\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Have to pass int, would break tests using TextReader directly otherwise :(\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/Trans_env/lib/python3.10/site-packages/pandas/io/parsers/base_parser.py:222\u001b[0m, in \u001b[0;36mParserBase._open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_handles\u001b[39m(\u001b[38;5;28mself\u001b[39m, src: FilePathOrBuffer, kwds: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m    Let the readers open IOHandles after they are done with their potential raises.\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/Trans_env/lib/python3.10/site-packages/pandas/io/common.py:702\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    701\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 702\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    710\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    711\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './bert_dataset/bert_train_data.csv'"
     ]
    }
   ],
   "source": [
    "PAD_IDX = 0\n",
    "def collate_batch(batch):\n",
    "\n",
    "   \n",
    "    bert_inputs_batch, bert_labels_batch, segment_labels_batch, is_nexts_batch,input_ids_batch,attention_mask_batch,original_text_battch = [], [], [], [],[],[],[]\n",
    "\n",
    "    for bert_input, bert_label, segment_label, is_next,input_ids,attention_mask,original_text in batch:\n",
    "        # Convert each sequence to a tensor and append to the respective list\n",
    "        bert_inputs_batch.append(torch.tensor(bert_input, dtype=torch.long))\n",
    "        bert_labels_batch.append(torch.tensor(bert_label, dtype=torch.long))\n",
    "        segment_labels_batch.append(torch.tensor(segment_label, dtype=torch.long))\n",
    "        is_nexts_batch.append(is_next)\n",
    "        input_ids_batch.append(input_ids)\n",
    "        attention_mask_batch.append(attention_mask)\n",
    "        original_text_battch.append(original_text)\n",
    "\n",
    "    # Pad the sequences in the batch\n",
    "    bert_inputs_final = pad_sequence(bert_inputs_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    bert_labels_final = pad_sequence(bert_labels_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    segment_labels_final = pad_sequence(segment_labels_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    is_nexts_batch = torch.tensor(is_nexts_batch, dtype=torch.long)\n",
    "\n",
    "    return bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_batch\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "train_dataset_path = './bert_dataset/bert_train_data.csv'\n",
    "test_dataset_path = './bert_dataset/bert_test_data.csv'\n",
    "\n",
    "train_dataset = BERTCSVDataset(train_dataset_path)\n",
    "test_dataset = BERTCSVDataset(test_dataset_path)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd2278a",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63119b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 10\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# Define the PositionalEncoding class as a PyTorch module for adding positional information to token embeddings\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # Create a positional encoding matrix as per the Transformer paper's formula\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: torch.Tensor):\n",
    "        # Apply the positional encodings to the input token embeddings\n",
    "\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf6ea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedding (nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_size ,dropout=0.1,train=True):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = TokenEmbedding( vocab_size,emb_size )\n",
    "        self.positional_encoding = PositionalEncoding(emb_size,dropout)\n",
    "        self.segment_embedding = nn.Embedding(3, emb_size)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, bert_inputs, segment_labels=False):\n",
    "        my_embeddings=self.token_embedding(bert_inputs)\n",
    "        if self.train:\n",
    "          x = self.dropout(my_embeddings + self.positional_encoding(my_embeddings) + self.segment_embedding(segment_labels))\n",
    "        else:\n",
    "          x = my_embeddings + self.positional_encoding(my_embeddings)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4b7514",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model=768, n_layers=12, heads=12, dropout=0.1):\n",
    "        \"\"\"\n",
    "        vocab_size: The size of the vocabulary.\n",
    "        d_model: The size of the embeddings (hidden size).\n",
    "        n_layers: The number of Transformer layers.\n",
    "        heads: The number of attention heads in each Transformer layer.\n",
    "        dropout: The dropout rate applied to embeddings and Transformer layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.heads = heads\n",
    "\n",
    "        # Embedding layer that combines token embeddings and segment embeddings\n",
    "        self.bert_embedding = BERTEmbedding(vocab_size, d_model, dropout)\n",
    "\n",
    "        # Transformer Encoder layers\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=heads, dropout=dropout,batch_first=False)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=n_layers)\n",
    "\n",
    "        # Linear layer for Next Sentence Prediction\n",
    "        self.nextsentenceprediction = nn.Linear(d_model, 2)\n",
    "\n",
    "        # Linear layer for Masked Language Modeling\n",
    "        self.masked_language = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, bert_inputs, segment_labels):\n",
    "        \"\"\"\n",
    "        bert_inputs: Input tokens.\n",
    "        segment_labels: Segment IDs for distinguishing different segments in the input.\n",
    "        mask: Attention mask to prevent attention to padding tokens.\n",
    "\n",
    "        return: Predictions for next sentence task and masked language modeling task.\n",
    "        \"\"\"\n",
    "\n",
    "        padding_mask = (bert_inputs == PAD_IDX).transpose(0, 1)\n",
    "        # Generate embeddings from input tokens and segment labels\n",
    "        my_bert_embedding = self.bert_embedding(bert_inputs, segment_labels)\n",
    "\n",
    "        # Pass embeddings through the Transformer encoder\n",
    "        transformer_encoder_output = self.transformer_encoder(my_bert_embedding,src_key_padding_mask=padding_mask)\n",
    "\n",
    "\n",
    "        next_sentence_prediction = self.nextsentenceprediction(transformer_encoder_output[ 0,:])\n",
    "        \n",
    "\n",
    "        # Masked Language Modeling: Predict all tokens in the sequence\n",
    "        masked_language = self.masked_language(transformer_encoder_output)\n",
    "\n",
    "        return  next_sentence_prediction, masked_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c046c2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 10\n",
    "\n",
    "# Define parameters\n",
    "vocab_size = 147161  # Replace VOCAB_SIZE with your vocabulary size\n",
    "d_model = EMBEDDING_DIM  # Replace EMBEDDING_DIM with your embedding dimension\n",
    "n_layers = 2  # Number of Transformer layers\n",
    "initial_heads = 12 # Initial number of attention heads\n",
    "initial_heads = 2\n",
    "# Ensure the number of heads is a factor of the embedding dimension\n",
    "heads = initial_heads - d_model % initial_heads\n",
    "\n",
    "dropout = 0.1  # Dropout rate\n",
    "\n",
    "# Create an instance of the BERT model\n",
    "model = BERT(vocab_size, d_model, n_layers, heads, dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cc0a5d",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9f10ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_fn_mlm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mevaluate\u001b[39m(dataloader\u001b[38;5;241m=\u001b[39mtest_dataloader, model\u001b[38;5;241m=\u001b[39mmodel, loss_fn_mlm\u001b[38;5;241m=\u001b[39m\u001b[43mloss_fn_mlm\u001b[49m, loss_fn_nsp\u001b[38;5;241m=\u001b[39mloss_fn_nsp, device\u001b[38;5;241m=\u001b[39mdevice):\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Turn off dropout and other training-specific behaviors\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_fn_mlm' is not defined"
     ]
    }
   ],
   "source": [
    "def evaluate(dataloader=test_dataloader, model=model, loss_fn_mlm=loss_fn_mlm, loss_fn_nsp=loss_fn_nsp, device=device):\n",
    "    model.eval()  # Turn off dropout and other training-specific behaviors\n",
    "\n",
    "    total_loss = 0\n",
    "    total_next_sentence_loss = 0\n",
    "    total_mask_loss = 0\n",
    "    total_batches = 0\n",
    "    with torch.no_grad():  # Turn off gradients for validation, saves memory and computations\n",
    "        for batch in dataloader:\n",
    "            bert_inputs, bert_labels, segment_labels, is_nexts = [b.to(device) for b in batch]\n",
    "\n",
    "            # Forward pass\n",
    "            next_sentence_prediction, masked_language = model(bert_inputs, segment_labels)\n",
    "\n",
    "            # Calculate loss for next sentence prediction\n",
    "            # Ensure is_nexts is of the correct shape for CrossEntropyLoss\n",
    "            next_loss = loss_fn_nsp(next_sentence_prediction, is_nexts.view(-1))\n",
    "\n",
    "            # Calculate loss for predicting masked tokens\n",
    "            # Flatten both masked_language predictions and bert_labels to match CrossEntropyLoss input requirements\n",
    "            mask_loss = loss_fn_mlm(masked_language.view(-1, masked_language.size(-1)), bert_labels.view(-1))\n",
    "\n",
    "            # Sum up the two losses\n",
    "            loss = next_loss + mask_loss\n",
    "            if torch.isnan(loss):\n",
    "                continue\n",
    "            else:\n",
    "                total_loss += loss.item()\n",
    "                total_next_sentence_loss += next_loss.item()\n",
    "                total_mask_loss += mask_loss.item()\n",
    "                total_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / (total_batches + 1)\n",
    "    avg_next_sentence_loss = total_next_sentence_loss / (total_batches + 1)\n",
    "    avg_mask_loss = total_mask_loss / (total_batches + 1)\n",
    "\n",
    "    print(f\"Average Loss: {avg_loss:.4f}, Average Next Sentence Loss: {avg_next_sentence_loss:.4f}, Average Mask Loss: {avg_mask_loss:.4f}\")\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9e3621",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4d8c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Masking(token):\n",
    "    \n",
    "    mask = bernoulli_true_false(0.2) # Decide whether to mask this token (20% chance); this is True or False\n",
    "\n",
    "    if mask is False: # If mask is False, return with '[PAD]' label\n",
    "        token_ = token\n",
    "        mask_label = '[PAD]'\n",
    "\n",
    "    random_opp = bernoulli_true_false(0.5)\n",
    "    random_swich = bernoulli_true_false(0.5)\n",
    "\n",
    "    if mask is True and random_opp is True and random_swich is True: # Replace the token with '[MASK]' and set label to a random token\n",
    "        token_ = '[MASK]'\n",
    "        mask_label = index_to_en(torch.randint(0, VOCAB_SIZE, (1,)))\n",
    "    \n",
    "    elif mask is True and random_opp is True and random_swich is False: # Leave the token unchanged and set label to the same token\n",
    "        token_ = token\n",
    "        mask_label = token\n",
    "\n",
    "    else: # Replace the token with '[MASK]' and set label to the original token\n",
    "        token_ = '[MASK]'\n",
    "        mask_label = token\n",
    "\n",
    "    return token_, mask_label\n",
    "\n",
    "def BERT_Masking(token):\n",
    "    # Decide if this token should be masked (15% chance)\n",
    "    apply_mask = bernoulli_true_false(0.15)\n",
    "\n",
    "    if not apply_mask:\n",
    "        return token, '[PAD]'\n",
    "\n",
    "    # Choose the masking strategy\n",
    "    rand = torch.rand(1).item()\n",
    "\n",
    "    if rand < 0.8:\n",
    "        # 80% of the time â†’ [MASK] in input, original token in label\n",
    "        return '[MASK]', token\n",
    "    elif rand < 0.9:\n",
    "        # 10% of the time â†’ original token in input, original token in label\n",
    "        return token, token\n",
    "    else:\n",
    "        # 10% of the time â†’ random token in input, original token in label\n",
    "        random_index = torch.randint(0, VOCAB_SIZE, (1,))\n",
    "        return index_to_en(random_index), token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ab7ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_mlm(tokens, include_raw_tokens = False):\n",
    "    raw_tokens_list = []  # List to store raw tokens if needed\n",
    "    bert_input = []  # List to store sentences processed for BERT's MLM\n",
    "    bert_label = []  # List to store labels for each token (mask, random, or unchanged)\n",
    "    current_raw_tokens = []\n",
    "    current_bert_input = []\n",
    "    current_bert_label = []\n",
    "\n",
    "    for token in tokens:\n",
    "        masked_token, mask_label = Masking(token)\n",
    "        masked_token, mask_label = BERT_Masking(token)\n",
    "\n",
    "        current_bert_input.append(masked_token)\n",
    "        current_bert_label.append(mask_label)\n",
    "\n",
    "        if include_raw_tokens is True:\n",
    "            current_raw_tokens.append(token)\n",
    "\n",
    "        # Check if the token is a sentence delimiter (., ?, !)\n",
    "        if token in ['.', '?', '!']:\n",
    "            # If current sentence has more than two tokens, consider it a valid sentence\n",
    "            if len(current_bert_input) > 2:\n",
    "                bert_input.append(current_bert_input)\n",
    "                bert_label.append(current_bert_label)\n",
    "                # If including raw tokens, add the current list of raw tokens to the raw tokens list\n",
    "                if include_raw_tokens:\n",
    "                    raw_tokens_list.append(current_raw_tokens)\n",
    "\n",
    "                # Reset the lists for the next sentence\n",
    "                current_bert_input = []\n",
    "                current_bert_label = []\n",
    "                current_raw_tokens = []\n",
    "            else:\n",
    "                # If the current sentence is too short, discard it and reset lists\n",
    "                current_bert_input = []\n",
    "                current_bert_label = []\n",
    "                current_raw_tokens = []\n",
    "\n",
    "    # Add any remaining tokens as a sentence if there are any\n",
    "    if current_bert_input:\n",
    "        bert_input.append(current_bert_input)\n",
    "        bert_label.append(current_bert_label)\n",
    "        if include_raw_tokens:\n",
    "            raw_tokens_list.append(current_raw_tokens)\n",
    "\n",
    "    # Return the prepared lists for BERT's MLM training\n",
    "    return (bert_input, bert_label, raw_tokens_list) if include_raw_tokens else (bert_input, bert_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3386a741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_losses = []\n",
    "eval_losses = []\n",
    "# Plotting the loss values\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(range(1,num_epochs+1), train_losses, label=\"Training Loss\", color='blue')\n",
    "plt.scatter(range(1,num_epochs+1), eval_losses, label=\"Evaluation Loss\", color='orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Evaluation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8cdd84",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03cdda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second sentence follows the first\n",
      "The cat sat on the [UNK].\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer with the BERT model's vocabulary\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "def predict_nsp(sentence1, sentence2, model, tokenizer):\n",
    "    # Tokenize sentences with special tokens\n",
    "    tokens = tokenizer.encode_plus(sentence1, sentence2, return_tensors=\"pt\")\n",
    "    tokens_tensor = tokens[\"input_ids\"].to(device)\n",
    "    segment_tensor = tokens[\"token_type_ids\"].to(device)\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        # Assuming the model returns NSP predictions first\n",
    "        nsp_prediction, _ = model(tokens_tensor, segment_tensor)\n",
    "        # Select the first element (first sequence) of the logits tensor\n",
    "        first_logits = nsp_prediction[0].unsqueeze(0)  # Adds an extra dimension, making it [1, 2]\n",
    "        logits = torch.softmax(first_logits, dim=1)\n",
    "        prediction = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    # Interpret the prediction\n",
    "    return \"Second sentence follows the first\" if prediction == 1 else \"Second sentence does not follow the first\"\n",
    "\n",
    "# Example usage\n",
    "sentence1 = \"The cat sat on the mat.\"\n",
    "sentence2 = \"It was a sunny day\"\n",
    "\n",
    "print(predict_nsp(sentence1, sentence2, model, tokenizer))\n",
    "\n",
    "def predict_mlm(sentence, model, tokenizer):\n",
    "    # Tokenize the input sentence and convert to token IDs, including special tokens\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    tokens_tensor = inputs.input_ids\n",
    "\n",
    "    # Create dummy segment labels filled with zeros, assuming it's needed by your model\n",
    "    segment_labels = torch.zeros_like(tokens_tensor)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass through the model, now correctly handling the output tuple\n",
    "        output_tuple = model(tokens_tensor, segment_labels)\n",
    "\n",
    "        # Assuming the second element of the tuple contains the MLM logits\n",
    "        predictions = output_tuple[1]  # Adjusted based on your model's output\n",
    "\n",
    "        # Identify the position of the [MASK] token\n",
    "        mask_token_index = (tokens_tensor == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
    "\n",
    "        # Get the predicted index for the [MASK] token from the MLM logits\n",
    "        predicted_index = torch.argmax(predictions[0, mask_token_index.item(), :], dim=-1)\n",
    "        predicted_token = tokenizer.convert_ids_to_tokens([predicted_index.item()])[0]\n",
    "\n",
    "        # Replace [MASK] in the original sentence with the predicted token\n",
    "        predicted_sentence = sentence.replace(tokenizer.mask_token, predicted_token, 1)\n",
    "\n",
    "    return predicted_sentence\n",
    "\n",
    "\n",
    "# Example usage\n",
    "sentence = \"The cat sat on the [MASK].\"\n",
    "print(predict_mlm(sentence, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db51b70d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec69f492",
   "metadata": {},
   "source": [
    "# 9) Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d1ea95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import multi30k, Multi30k"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Trans_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
